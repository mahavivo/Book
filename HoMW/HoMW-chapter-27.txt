Chapter 27

THE CHANGING MODERN WORLD

129. Western Europe after the Cold War

130. Nation-States and Economies in the Age of Globalization

131. Intellectual and Social Transitions in Modern Cultures

132. International Conflict in the Twenty-First Century

133. Social Challenges in the Twenty-First Century

The demise of Soviet-style Communist regimes in Europe after 1989-1991 suggested to some observers that the history of the modern world would henceforth evolve in only one direction. All modern societies, according to such theorists, were in fact already moving on their own erratic paths toward a universal system of liberal democracies and free market economies. But it soon became evident that this imagined “end of history” was by no means the only direction in which history could evolve. New movements arose to challenge the global capitalist economy, the ascendancy of Western political theories, the secularism of modern cultures, and the powerful influence of the United States—which had emerged from the Cold War as the world’s sole superpower. There were new and continuing conflicts among competing religious or ethnic groups and among nations that competed for power and commercial advantages in the global economy. Warfare itself changed when militant, extremist groups increasingly dramatized their grievances or waged violent political campaigns through the indiscriminate tactics of terrorist bombings. Facing such opponents, the most powerful national governments found that traditional methods of international warfare-invasions, the surrender of enemy governments, military occupations—could no longer achieve the decisive political and military closure that modern nation-states expected to reach at the end of their wars.

No sooner had the Cold War receded into historical memory than global straggles over natural resources, economic interests, and cultural values began to shape a new series of international conflicts and wars. By the early twenty-first century, the United States and some other Western nations had become deeply involved in the long-simmering conflicts of the Middle East, preoccupied by the dangers of terrorism, and entangled in protracted military campaigns against Islamic groups in Afghanistan and Iraq. The much-anticipated post-Cold War “peace dividend” disappeared almost immediately in a new cycle of military interventions, shadowy intelligence operations, and deadly terrorist attacks.

Amid these cycles of violence and conflict, however, the deeper historical patterns of human migration, global economic exchange, technological innovation, transnational cultural interaction, and changing social mores continued to appear in all regions of the world. Computer technologies and communications systems now moved information around the globe with instantaneous speed; a global “culture industry” spread the same music, films, foods, and fashions throughout the world; evolving conceptions of knowledge and scientific research reshaped intellectual debates as well as artistic creativity; and new opportunities for the education, professional careers, and social and political rights of women continued to expand in almost every modern nation. The pace of change seemed to be increasing in all spheres of modern life, creating a “global village” that was both united and divided by the processes of contemporary globalization.

129. WESTERN EUROPE AFTER THE COLD WAR

Economic and Political Uncertainties in the New International Context

It was generally expected that the nations of central and eastern Europe would at first suffer in the post-Communist transition to democracies and market economies. What was unanticipated was the burden of economic and political troubles that western Europe found itself confronting. Buoyant and self-confident because of continuing prosperity during the 1950s and 1960s, the people of western Europe were first jolted by the recession of the 1970s that combined economic stagnation with inflation. By the mid-1980s the European democracies had recovered, although with lower growth rates than in the past and with discomfiting levels of unemployment.

In Germany the economic situation after 1989 was complicated by the reunification with East Germany. The steep costs of absorbing East Germany’s decayed economy and the decision to convert the East German mark on an equal basis with the West German made inflation a threat. To combat it the German Bundesbank kept interest rates high. But tight money choked off credit and investment when downturns occurred as in the early 1990s, delaying economic recovery for itself and its European neighbors and leading to currency devaluations and instability. The German economy remained sluggish long after political unification was completed.

Despite recovery, many West Europeans had remained unemployed. By the early 1990s unemployment rose to postwar highs. For the members of the European Union the unemployment figure approached 19 million, or more than 12 percent of the workforce. Long-term unemployment became a recognized phenomenon, as well as the disconcerting corollary that many workers would never again be employed at their old skills. Young people especially were hurt by shrinking job prospects.

The Europeans slowly came to realize what had only been suspected in the earlier recession, that the problem of unemployment was not temporary, limited to the business cycle, but was deep-seated and structural. High labor costs that included generous welfare-state benefits for unemployment, disability, retirement, and extended paid vacations undercut western Europe’s ability to compete globally. Many corporations, like their American counterparts, began to restructure, sharply reducing employment rolls and introducing other economies. American and European multinational corporations shifted manufacturing operations to developing countries in Asia and other parts of the world where labor costs and benefits were lower. For the first time in the years since 1945 the West European nations, echoing the British example under the Thatcher government, took steps to roll back the welfare state as it had evolved by consensus since the end of the Second World War.

Western Europe: Political Crises and Discontents

There was widespread dissatisfaction with the governing political parties of the center and center-left, which were judged to have held office too long and seemed incapable of providing fresh vision for the future. Shocking revelations of scandal and corruption tarnished the Christian Democratic and Socialist parties in several major countries.

The rising dissatisfaction with European political cultures could perhaps be seen most clearly in Italy, where anger against the governing elite became symptomatic of the wider European response to political cronyism. For years the Christian Democrats had dominated the political scene, forming successive coalition governments, with the Right in the 1950s and with the Socialists and other parties in the 1960s. By the 1970s in the midst of economic setbacks their popular support shrank rapidly. In the 1980s the Socialists replaced them, heading a cabinet for a record four years. Meanwhile the country’s second strongest party, the Italian Communist party, which had earlier demonstrated its independence from Moscow, was excluded from government at the national level but won numerous mayoralties and control of municipal councils. In 1989, with the collapse of communism and end of the Cold War, the party renamed itself the Democratic Party of the Left and began a new quest for participation in the national government.

Over the years the Christian Democrats had kept a tight grip on power and patronage and maintained close ties both to state-owned enterprises and private business. In good part because the Christian Democrats projected themselves as Italy’s defenders against communism they were shielded from scrutiny or criticism. But in the early 1990s evidence surfaced of bribes, kickbacks, and payoffs for government contracts up to the highest political levels. There had been illegal payments to the political parties and even widespread collusion with organized crime. Former prime ministers, cabinet ministers, and parliamentary deputies of all political parties as well as top business executives were implicated. So tainted was the Christian Democratic party that it returned to an earlier name for itself, the Popular Party. The aroused electorate in 1993 approved a new electoral system, one feature of which was to abandon proportional representation.

In the elections in 1994 both the former Christian Democratic party and the Socialist party were routed. Three new parties won and formed a Center-Right coalition. But the new parties inspired little confidence and their political leaders were unable to provide stability or serve as civic models. All the governments inherited public deficits, economic slowdown, unemployment, social tensions, and the continuing corruption inquiries. In 1998 the Popular Party joined with the Democratic Party of the Left to form a cabinet, and Massimo d’Alema, the head of the former Communist party, became prime minister. The reconstructed Communists were committed to financial stability, private enterprise, and economic growth as the best means of advancing social needs. But this cabinet lasted only briefly. In 2001 the conservative media magnate Silvio Berlusconi headed a coalition that won the elections; its Center-Right majority promised stability but also aroused misgivings about the ambitious prime minister and his assertive Forza Italia party (the name itself derived from the championship soccer team that he owned). Berlusconi nevertheless remained in office longer than most of his predecessors. Fending off charges of corruption and criticism of his support for the U.S.-led invasion of Iraq, he overcame losses in a number of elections and managed to retain power in the face of mounting domestic opposition. In 2005 he reintroduced proportional representation in order to divide his opponents.

Europe's Immigrants and Refugees

We have already seen how the influx of millions of immigrants and refugees since the 1960s was altering the nature of European societies. In the 12 years from 1980 to 1992,15 million new immigrants arrived to make their home in western Europe. Some sought political asylum, but mainly they came in search of wider economic opportunities than could be found in their own impoverished countries. They were usually willing to fill the lower-paid, less-desirable jobs that the Europeans did not want. When the Cold War ended and Communist barriers came down, a flood of refugees left eastern Europe, and additional refugees streamed in from war-torn Yugoslavia.

The steady flow of immigrants visibly changed the ethnic composition of Europe. In Germany Turkish workers settled in with their families as permanent residents; many reared their children as Muslims. In France in a population of 57 million in the mid-1990s, 4 million, or 7 percent, were foreign-born—mainly Arabs from North Africa and immigrants from other former French African colonies but also Vietnamese from Southeast Asia. In Britain, with a population of 56 million, at least 2.5 million could be counted as “ethnic minorities,” mainly from various parts of Asia, Africa, and the Caribbean. Nations that had long sent emigrants to all parts of the world now found themselves absorbing large new populations from abroad.

The xenophobic hostility which had manifested itself earlier in western Europe again flared up. In Germany there were physical assaults on Turks, firebombings, and other acts of violence. Neo-Nazis in Germany, “skinheads” in Britain (who dated from the 1950s), neo-Fascists in Italy, and followers in France of Jean-Marie Le Pen’s National Front agitated the immigrant issue and even resorted to racist violence. Governments adopted laws to reduce or eliminate further immigration. Germany in 1993 repealed a constitutional provision that had offered asylum to “all persecuted on political grounds,” but it became virtually impossible to distinguish between political and economic refugees. France moved toward “near-zero” immigration and abandoned a tradition going back to the Revolution of granting citizenship to any child born on French soil. Unemployment and troubled economies made the social tensions potentially explosive and fed prejudices of all kinds, including anti-Semitism. But western Europe was only one outpost of a global crisis of refugees and migrants on the move.

Western Europe seemed to be settling into an era of persistent gloom, pessimism, and political frustration during the early 1990s. The end of the Cold War had not automatically ushered in peace, prosperity, and harmony. Unemployment, especially for the young, marred the economic scene. In the euphoria following the collapse of communism, few had anticipated the resurgence of the explosive nationalism that brought bloodshed to the Balkans. Western Europe, for all its announced commitment to democratic institutions and for all its military strength, feared the entanglements of intervention so much that until the American-led air offensive in Kosovo it refrained from decisive action. On the political scene conservative and centrist political leaders offered few new initiatives, and the Left seemed in disarray. On the other hand, many still believed, or hoped, that western Europe would reach out to central and eastern Europe, continue to work toward a united Europe, and reassume a central role in the world’s economy and international affairs. But even the most optimistic Europeans increasingly recognized that the pursuit of such goals would require bold new economic or political initiatives.

130. NATION-STATES AND ECONOMIES IN THE AGE OF GLOBALIZATION

Economic Recovery and a "Third Way" in Politics

In the 1990s the American President Bill Clinton set the tone for a new type of democratic politics. Taking his lead from Republican pro-business policies, Clinton promoted a program that favored economic growth and productivity but combined it with such social issues as health care and education and concern for the disadvantaged and minorities. He was favored by an upturn in the business cycle. In 1992, the American economy began to embark on a peacetime expansion that exceeded the previous nine-year record of the 1960s. Unemployment dropped to an all-time low, trade and production expanded, the stock market rose to unprecedented heights. The economy benefited from earlier corporate cost-cutting and restructuring as well as from the emergence and growth of the revolutionary new computer industries and information economy. Before long the global economy, buoyed by American prosperity, followed suit despite financial crises in Asia, Latin America, and elsewhere. Clinton also pursued an activist foreign policy, especially in the Arab-Israeli conflict in the Middle East. His second term as president was marred, however, when his denial of sexual indiscretions with a young woman intern led to a vote of impeachment by the House of Representatives; the Senate after a trial came short of the two-thirds vote needed for his removal from office.

Clinton found a kindred political spirit in the buoyant British Labour Party leader Tony Blair, who also adopted many conservative pro-business policies. Blair persuaded his party to abandon what still remained of its earlier socialist and welfare-state platform. Neutralizing the party’s older trade union and left-wing bastions, he projected an image of the party as “New Labour.” The best prescription for prosperity was to encourage economic growth and industry but without satisfying “pure selfish individualism” or neglecting social needs, which he accused Thatcher and the Conservative party of doing. A market economy and private enterprise would unleash productive forces that would benefit everyone. After four Conservative electoral victories since 1979 and 18 years of Conservative leadership, the country gave the Labour Party in 1997 its largest margin of victory since 1945.

As in the United States, the economy turned upward in Britain. Blair’s political strength and popularity also enabled him to institute a number of constitutional changes. The United Kingdom had long had a highly centralized government, with all power emanating from the Westminster Parliament in London. The Labour party with its large majority in the House of Commons took unprecedented steps toward a devolution of power to a new parliament in Scotland, where the reform was intended to defuse a strong separatist movement, and to a new legislative assembly in Wales. Each gained broad jurisdiction over its internal affairs. Voters in Scotland for the first time since the Act of Union of 1707 went to the polls to elect their own parliament, and in Wales voters elected an assembly for the first time ever. Critics saw the devolution of authority as the “undoing of Britain” and an erosion of parliamentary sovereignty. Others recognized that stability did not necessarily mean resistance to change. No one wished to repeat the mistakes of earlier years when Britain’s resistance to home rule for Ireland had led to continuing violence, partition, and prolonged strife.

In Northern Ireland matters seemed to be taking a more peaceful turn. The Protestant and Catholic parties reached an agreement in 1998, moving to end a violent conflict that over the past 30 years had cost the lives of over 3,000 people. The new arrangements authorized a legislative council of representatives from Britain, Northern Ireland, and the Republic of Ireland, and a sharing of power in a joint cabinet. Northern Ireland thus gained a new opportunity for self-government, but the introduction of the new self-governing institutions was delayed when the Irish Republican Army (IRA) and other paramilitary groups resisted calls for complete disarmament. By 2005, however, the IRA had in fact given up its weapons, and there was reason to believe that the long, violent conflict between Catholics and Protestants might finally be giving way to a more peaceful era in Northern Ireland’s politics and society.

In another momentous break with tradition a reform bill in 1998 stripped the hereditary peers in the House of Lords of their parliamentary seats. The hereditary seats, it was widely recognized, represented a political anachronism, untenable in a contemporary democracy. A reorganized House of Lords would consist only of lifetime peers. Many in Britain were opposed, but others placed the reform in the established tradition of broadening the suffrage and other democratic innovations.

In Germany and in France political leaders also represented the new politics of the Left and Left-Center. In Germany the Social Democratic Chancellor, Gerhard Schroder, replaced Helmut Kohl in 1998 after the Christian Democratic chancellor’s 16 years in office. Schroder, whose party’s moderate wing had triumphed over its more radical elements, was committed to curbing the generous provisions of the postwar welfare state, which had driven up labor costs and made it difficult to hire new workers. He too spoke of his program as a “middle way,” and—despite the problem of chronic unemployment—the Social Democrats held on to power in the elections of 2002. Subsequent elections in 2005, however, failed to produce a clear majority for either of the major German parties, and a coalition government emerged after extended negotiations. Angela Merkel, the leader of the Christian Democrats, became not only Germany’s first woman chancellor but also the first chancellor to reach that office from the former East Germany. Half of the cabinet ministries went to Social Democrats, however, creating an unusual Left-Right collaboration.

In France Daniel Jospin, a long-time moderate Socialist leader who hoped to continue President Mitterrand’s reformist policies, became prime minister in 1997. Two years earlier he had lost the presidential elections to his conservative Gaullist opponent Jacques Chirac, but the Socialists soon won a majority in the legislature; under the now established practice of “cohabitation” the president appointed Jospin prime minister. Jospin, though still resolutely committed to a large role for government on social issues, believed in moderation and pragmatism, favored a market economy and moved forward with the privatization of state-owned industries. The constitution meanwhile was amended to curtail the powerful role of the Gaullist-inspired French presidency by reducing the term of office from seven to five years. The Socialist party was now so fragmented, because of its dissatisfaction with Jospin, that in the 2002 presidential election it could not muster enough votes for Jospin even to remain a candidate in the second round of the election. It was the extremist right-wing candidate Jean-Marie Le Pen who faced Chirac. Leftist and other voters, to their consternation, had to rally to Chirac, who easily won reelection.

Despite such setbacks in various national elections, the political leaders who advocated a “middle” or “third” way between conservatism and welfare-state socialism continued to promote a moderate reform agenda in the early twenty-first century. Although they insisted that governments should play an active role in dealing with social and economic problems, they also called for reduced public spending, budget balancing, limits on regulation, and a climate of opportunity for business enterprise. Pragmatism and moderation seemed to be triumphant over older socialist views and the welfare-state ideology that had prevailed in the years after the Second World War.

Japan in the 1990s

The 1990s were an economic and political watershed for Japan. Its economy had expanded almost uninterruptedly since the postwar years. In the 1970s and 1980s capital investment in industry, and especially in advanced technology, grew at times at twice the rate of American and West European investment. But in the summer of 1991 Japan suffered a severe financial crisis and sank into a deep and stubborn recession. The downturn signaled a problem that was more than a swing in the business cycle.

The country was paying the price for years of uncontrolled speculation at home and abroad. Investors, including the largely unregulated banks, had made risky forays into real estate and financial markets. Government control over monetary policy had been lax. A very rapid rise in the price of stocks and real estate had created a “bubble economy,” not unlike the economy of the 1920s in the United States that had led to the Great Depression. When the price bubble burst in 1991, Japan suffered bank failures and bankruptcies, and as the downturn continued, the government unsuccessfully tried to revive the economy with belated and ineffective fiscal and monetary policies. Japan’s export economy added to its difficulties. Over the years the country had exported freely while sharply restricting imports, and other nations finally rebelled. New low-cost competition was now invading Japan’s own markets. Japan also was forced to deal with social pressures. Although wages, educational opportunities, and living standards remained high, housing was scarce and expensive and the country’s infrastructure suffered from neglect.

In the midst of its economic troubles a political crisis, unprecedented in the years since the Second World War, shook the country. Cumulative evidence mounted of bribery, corruption, and ties to organized crime among high-ranking political and business leaders. In 1993 the Liberal Democratic party, which had dominated political life and served as the political pillar of Japan’s prosperity and stability, for the first time lost its parliamentary majority in the Diet and fell from office. New faces and new parties emerged to replace the old guard, even though the country’s bureaucracy remained entrenched. The recession dragged on and currency and financial crises in much of Asia in 1998 set recovery back, but the economy showed signs of renewed growth by the middle of the following decade. Unemployment began to decline, stock prices rose again, and industrial investments steadily increased. The crisis and prolonged recession had transformed the country, but Japan was not to be counted out as one of the world’s leading political and economic powers. Efforts at political reform stimulated a reexamination of the past and aroused hopes for a revived economy, a more democratic future, and a more flexible society. But in the early twenty-first century Japan had somewhat faded as the model of postwar industrial and financial success and served instead as a cautionary example of a “bubble economy” that had burst.

The European Union: Widening and Deepening

By the opening of the twenty-first century European integration had made remarkable progress since six West-European nations had created the Common Market in the 1950s. The European Economic Community, established in 1957, became the European Community in 1965, and it in turn became the European Union (EU) under the Treaty of Maastricht signed in 1991. Having grown to 25 member nations after 2004 (with more countries seeking to join), the EU was an economic superpower. It had become the world’s largest single economic market with over 440 million people, closely bound together in noneconomic ways as well.

As agreed under the Maastricht treaty, the EU embarked in the 1990s on one of its most ambitious enterprises, the voluntary adoption of a common currency. Twelve EU nations began to use the new “euro” in 2002 for financial transactions and the cash payments of daily life, replacing such historic national currencies as the franc, the mark, and the lira. The euro’s value fluctuated in global financial markets, but it quickly became a major international currency and competed strongly against the American dollar. Three EU members—Britain, Sweden, and Denmark—chose not to adopt the euro, and the ten new states that joined the EU in 2004 remained outside the euro currency zone, pending further internal economic reforms. In another unprecedented step a European Central Bank was established in Frankfurt with the authority to set monetary policy for the member countries. The effects of this monetary policy on each country’s social and economic policies became a new controversial issue in European politics as the various national economies came under a more centralized system of monetary management.

Meanwhile the requirements for admission to the European Union remained firmly attached to liberal political and economic models. An applicant nation had to demonstrate its commitment to “liberty, democracy, respect for human rights and fundamental freedoms, and the rule of law.” A functioning free market economy was also required. In contrast to its slow expansion earlier, the EU now welcomed all European nations meeting its specifications, and it was also considering the possible admission of Turkey.

The EU was also “deepening” in additional ways. The Maastricht treaty had called for the continued development of common foreign and defense policies. The failure of the Europeans to prevent or halt the bloody events in Bosnia or in Rwanda in the early 1990s and the later failure to thwart Serbia’s assault on Kosovo had embarrassed the Europeans. In June 1999 the EU foreign ministers resolved to develop a capacity for collective military action of their own. The EU began to move toward creating an armed force supplementary to but not supplanting NATO. The Atlantic alliance would remain firm, but a new “European Defense Agency” came into full operation after 2005. The new military cooperation signaled Europe’s intention to end its long-term dependence on the United States and to reject the status of “an American protectorate.” These developments both pleased and disturbed American policy makers who wanted Europe to play an independent role, especially in European matters, but did not wish to see resources drained off from NATO.

The EU political and bureaucratic structures did not change as rapidly as the evolving economic or defense policies. The European Commission, with an extensive civil service bureaucracy in Brussels, served as its executive and administrative branch and had the right to initiate proposals for legislation. The European Parliament, elected by a Europeanwide electorate, supervised the budget and debated Commission proposals. This representative body never aroused much enthusiasm, and voter turnout in the member nations at election time remained low. Although the heads of government held regularly scheduled meetings, the foreign ministers of the member-nations meeting as a council convened frequently to discuss and vote on matters of common interest. The meetings of this council provided an opportunity for the EU to take positions on international issues that extended well beyond Europe itself. It had not been anticipated that the ministers would evolve into the chief policy-making body, but in some ways it tied the EU to democratic support in a way that the European Commission had failed to do.

The early impetus for supranationalism in the Union faded, but what remained was the close cooperation of the ministers representing the member-states, discussing and making decisions on the basis of a half-century of common values and experience and a large body of jurisprudence and legal principles under the jurisdiction of the Union’s Court of Justice. Each nation saw its stability, prosperity, and security enhanced by membership in the Union and recognized that the loss of sovereignty had been less than feared.

But there was strong opposition to a proposed EU constitution that would have created the framework for a more coherent, integrated political union. Although some countries supported the proposed constitution, voters in France and the Netherlands rejected the proposal in 2005, throwing open the whole question of how the EU might proceed with further integration. Many Europeans favored the proposed constitution, particularly for its broad guarantees of social and economic rights and entitlements, but many others expressed concern about perceived threats to European living standards and social values or to specific national interests— especially after the major expansion of the EU membership in 2004. The European Union was therefore not likely to develop in the near future as its more visionary advocates had anticipated, even if in its present form it had become an unprecedented example of economic and political cooperation that fundamentally altered Europe. After centuries of European warfare, the continent had reached a historical turning point in which war between the leading European powers seemed unimaginable.

French-German cooperation, which had been the original driving force behind the success of European integration, still remained crucial. Many people in both countries looked forward to an even closer French-German alliance, partly to give leadership to the newer EU members from Eastern Europe and partly to offset British influence. No one could predict, however, the future evolution of an enlarged and still growing European Union that could be seen as a potential alternative or even rival to the global political and economic power of the United States. In Europe the older strategic threat from the Soviet Union was gone, but many Europeans now saw a threat to the European way of life in American-led “globalization,” which Europeans criticized as a worship of mass consumerism and a materialistic abandonment of social and cultural values. To Europeans, even among admirers of American accomplishments, the EU provided an essential balance to American hegemony in world affairs.

The"New Economy": The 1990s and Beyond

The global economy that reemerged after the Second World War seemed to enter a new phase in the 1990s. Globalization was the key word. It meant the more rapid and efficient movement of capital and technology worldwide across all geographical and political boundaries. The United States took the lead in globalization and in the development of what came to be known as the “new economy,” but large multinational corporations of other nations also had operations in all regions of the world.

The new economy was an information economy. At its base was the computer revolution and the Internet, a word scarcely known at the decade’s beginning. By the late 1990s it was in everyone’s vocabulary, and at the disposal of tens of millions in offices and homes worldwide. It provided instant access to information of all kinds. It was as if a new industrial revolution was setting in and at an accelerated speed exceeding all previous industrial changes. Trade expanded. The flow of private investment capital from the industrial nations to the developing countries quickened. Living standards in many parts of the world rose, all tied to growing trade and investment. Globalization, driven by the profit motive, arguably held greater potential for Third World countries than foreign aid or loans from governments and international agencies.

The United States was the driving force in the new global economy. The American economy began an initial period of quiet growth and expansion in 1982. The inflation of the 1970s had been brought under control by high interest rates imposed by the Federal Reserve Board, the nation’s politically independent central bank, which continued its periodic interventions to adjust to economic changes. There were setbacks and brief recessions, but by 1992 the economy began a period of uninterrupted growth. Unemployment declined to 4 percent, the lowest since the 1960s. Government revenues were available for many purposes, and some were used to reduce the national debt. American productivity, aided by computer technology, rose. Inflation with respect to consumer prices, although not for equities and real estate values, was kept in check. Consumer confidence and purchases, buoyed by the rising values of stocks and homes and the availability of credit, remained high. The long boom finally stalled in the early years of the new century when both the stock market and the rate of economic growth began to decline. By that time, however, the country had experienced the longest period of uninterrupted economic expansion in its history.

But there were troubled areas elsewhere in the industrial world. Japan, still suffering from a long severe recession after its financial collapse in 1989, only began to recover at the end of the 1990s; and there were continuing crises in Russia’s unsettled economy, including the drastic devaluation of the ruble in 1998. Especially serious were currency and financial crises in 1997-1998 in several of the hitherto golden economies of Asia. The troubles there began with a loss of confidence and collapse of the currency in Thailand and led to a withdrawal of foreign investment that sent currencies tumbling in Malaysia, South Korea, Indonesia, Hong Kong, and Singapore—the very countries that had helped create the “Asian miracle.” Speculation in real estate and equities in these countries had gone wild. Imprudent business practices came home to roost.

Responding to Asia’s economic problems, the IMF made multibillion-dollar loans but imposed austerity measures that brought social hardships and contributed to new tensions and difficult adjustments within various Asian nations. Many workers feared that the economic opportunities of globalization would be offset by the continuing quest for cheap labor or by a breakdown in the services or assistance available for the poorest social classes. To enhance competitive power in the global marketplace large corporations in the United States, Europe, and Asia were turning to low-wage labor in poorer countries and buying other companies to become even larger, within and across national lines. Multinational corporations grew in numbers and size everywhere, and there seemed to be little concern over foreign companies’ taking over domestic corporations. London regained much of its financial prestige and strength as the financial center for European mergers and amalgamations, which accelerated as never before.

As the American stock market boomed in the 1990s and new computer technology companies soared in value, some observers recalled the prosperity and speculative excesses of the 1920s and the American stock market crash in October 1929. Despite warnings from some quarters against “irrational exuberance,” stock values kept climbing. Some also saw warnings in the collapse of the Japanese bubble after 1989. Still others recalled past speculative fever and manias in history: tulips in the seventeenth century; colonization schemes in the eighteenth; canals, railroads, gold, and silver in the nineteenth.

The computer revolution and the emergence of the new computer technology companies created a new economy in many ways. They helped the economy to grow through greater productivity and helped fuel the rising prices in global stock markets. Computer wizards as well as brokers, financial executives, and entrepreneurs of all kinds became wealthy, often at an early age. Inventions in makeshift garage workplaces by hitherto obscure individuals, backed by venture capital, led to the establishment of multibillion-dollar corporations. The president of Microsoft, Bill Gates, became the richest man in the world, counting a fortune in the billions.

The new global economy generated unprecedented wealth, but the vast system of industrial production and financial exchange was also vulnerable to regional economic crises or political upheavals, all of which could now affect economies around the world. Optimistic economists responded to the Cassandras who worried about a future financial “crash” by arguing that this was a “new economy” and that market prices would be sustained on a new high plateau. But some economists had made somewhat similar predictions in 1929. There were indeed grounds for believing that the market did reflect profound changes in the American and global economy closely related to increased productivity, but there were warning signs in the early years of the new century that speculative fever had gone too far. Shock waves reverberated throughout the economy when high-technology computer stocks, which had risen in price all out of proportion to their earnings, lost much of their value in the early years of the new century, and real estate values began to soften.

The two most important developments in the computer revolution of the 1990s were the Internet and the World Wide Web. The Internet began from modest beginnings in the late 1960s in the United States as a government-sponsored project to enhance communications. Computer scientists around the world enlarged its scope and made it easier for individuals and companies to utilize in their daily work. The World Wide Web was first developed in 1980 at a nuclear physics research center near Geneva when the young English physicist Tim Berners-Lee developed a program to process information through electronic associations and linkages. For a time it remained neglected, but by 1990 he and others had revived and perfected it. By means of the Internet, the World Wide Web became a mass medium navigated by hundreds of millions around the world every day, and its users continued to multiply. It revolutionized communications; people could send electronic mail to each other, negotiate financial transactions, make purchases, tap all kinds of sources for information and education, and use the Web for entertainment as well. Many compared the significance of the new computer technology to the invention of the printing press in the fifteenth century. So rapidly was the new technology developing in speed, efficiency, and versatility that new, more powerful computer models became available almost immediately after earlier ones had first come into use.

Nearly half of the world’s Internet users at the beginning of the new century were to be found in the United States, and English became the dominant language of the Internet. Some Europeans and Asians expressed concern that the Internet represented not only an immense step toward globalization but also toward Americanization. However, global competition continued. A Finnish company held the lead in the cellular phone market, computer specialists developed innovative software programs in India, and companies in Taiwan manufactured many of the most advanced computer chips. But a serious gap opened up meanwhile between those who had the opportunity and means to avail themselves of the new computer technology and those who did not. A “digital divide” emerged both within countries and between the wealthy and poorer countries of the world.

Many raised questions about the spectacular American economy of the 1990s and its impact on American society. The economic boom seemed to have left those at the bottom of the economic ladder falling behind. The gap between rich and poor was widening. In the years from 1988 to 1998 the income of the wealthiest fifth of American families rose by 15 percent as against less than 1 percent for the poorest fifth. The incomes of the poor and of many working-class and middle-class families fell or stagnated even when entire families worked. The gap grew wider between those who could save, invest, and build more wealth and those who could not. More Americans were living below the poverty line in 1999 than in 1969, and the economic gap between the rich and poor continued to grow in the first decade of the new century. Lack of opportunity for education and training in the new skills presented a new kind of challenge for the poor and for workers whose jobs disappeared in the global quest for lower-cost labor.

The worldwide faith in the market and market forces presented other challenges as well. Faith in a kind of oversimplified laissez-faire market economy spread globally, a kind of “market fundamentalism,” not unlike a religious faith. The invisible hand would make it all right for everyone in the end, its supporters said, and a rising tide would lift all boats. Globalization and the world’s market economy, its advocates claimed, made intervention by governments unnecessary. The argument found critics everywhere. In France, where unemployment was high, some argued that it was immoral for corporations to earn large profits while at the same time laying off workers in what were described as “abusive job cuts.”

Meanwhile the growth in world commerce led to the creation in 1994 of a more formal World Trade Organization (WTO) to replace the General Agreement on Tariffs and Trade (GATT) in effect since 1948. GATT had been immensely successful in lowering tariffs and enlarging world trade through informal bargaining procedures and agreements. By 1986 it had grown to include 92 nations. In 1995 the United States succeeded in creating a permanent, more formal organization. The WTO was authorized to draw up and oversee agreements, enforce trade rules, and settle disputes. It grew to 150 participants by 2005 but at its first new-style open bargaining sessions in Seattle, it met with opposition and demonstrations in the streets, as would soon become the pattern at subsequent meetings elsewhere. The negotiated free trade agreements, the critics protested, paid little attention to environmental and labor conditions, or even to the widespread exploitation of child labor in large parts of the developing world. The developing nations, for their part, saw themselves at risk of losing out on cheaper labor costs, one of their chief competitive advantages; and they pointedly underscored the large share of ecological damage for which the industrial nations were responsible. Subsidies to agriculture in wealthier countries placed the less industrial nations, dependent on either the export or import of food, at a disadvantage. Critics of the new international trade system rejected arguments for free trade as a means of enhancing living standards for all. The violence of the popular protests offended many, but they aroused new sensitivities to social issues worldwide.

Globalization, already visible in earlier periods of the world’s history, and especially so in the nineteenth and twentieth centuries, seemed to be the defining social and economic theme for the new century. National boundaries, though not disappearing, were increasingly transcended by cross-boundary transactions and global exchanges in the world of communications, industry, culture, travel, food, popular entertainment, even apparel. To some, globalization was still thought of as “Westernization” or “Americanization.” By the early twenty-first century, however, it defined only the latest phase of modernization, shared by many more of the world’s people than ever before. The giant multinational corporations, no longer exclusively European or American but home-based in nations all over the world, depended as never before on the movement of capital, goods, trained personnel, technology, and ideas that flowed across traditional national boundaries.

131. INTELLECTUAL AND SOCIAL TRANSITIONS IN MODERN CULTURES

Intellectual and cultural life does not usually evolve through the dramatic public events or conflicts that reshape politics or economic relations. Yet the transitions in modern intellectual life and social mores have transformed modern history during all the years of international conflict and revolutionary change that we have discussed. People define themselves and their cultures in activities that go far beyond politics and economics. They constantly develop new forms of knowledge, philosophy, religious belief, creative art, and social life, all of which both influence and respond to the transformations of the modern world. Although much of twentieth-century culture had its origins in the years 1871 to 1914, science, philosophy, the arts, and religion crossed new frontiers or took new directions over the course of the twentieth century and set the stage for the new century. Even to single out a few of these developments will suggest the vast changes of the contemporary era.

The Advance of Science and Technology

Science and technology expanded rapidly in the half-century before the First World War, but the pace quickened in the following years. Scientific discovery advanced more rapidly in the years after 1919 than in all previous human history. For one thing, more scientists were at work. At the opening of the twentieth century, about 15,000 scientists were exploring scientific problems; in the latter half of the century, over a half-million scientists were engaged in research, more than in all previous centuries combined. Over 85 percent of all scientists who have ever lived are at work in our own age.

The average person experienced the triumphs of science most dramatically in medicine and public health. Nothing in previous medical discovery could equal the contributions of sulfa drugs, antibiotics, cortisone, and other substances used to combat formerly crippling or deadly diseases, including pneumonia and tuberculosis; hormones, adrenaline, and insulin were also available to promote health or relieve suffering. Vaccines combated a number of dread diseases, including, after 1955, poliomyelitis; by 1975 smallpox had been all but eradicated worldwide. Remarkable accomplishments in surgery included the transplanting of vital organs. Apart from the advances in medical science, people benefited from modern technology in almost every aspect of their daily lives. For entertainment, radio and the motion picture were available and, after the Second World War, television. Washing machines, freezers, frozen foods, and microwave ovens lightened household duties. After 1947 airplanes could fly faster than the speed of sound; giant aircraft could traverse huge distances in a few hours; tourist travel to distant parts of the earth became commonplace. A world of electronics, robotics, rocketry, and space technology opened. Many people came to believe that technological solutions could be found for every problem.

It was therefore a shattering experience when the fatal disease AIDS (Acquired Immuno-Deficiency Syndrome) appeared in the early 1980s and by the 1990s was assuming global epidemic proportions. The first case was reported in the United States in mid-1981, but the disease seems to have originated earlier and elsewhere. Deaths from the disease mounted, and it was estimated that tens of millions had the disease or were infected with HIV, the human immunodeficiency virus that caused it. While medical scientists worked in the early twenty-first century to devise a preventive vaccine or to prolong the lives of its victims, educational efforts to stop its spread focused on sexual practices, intravenous drug use, and the protection of blood supplies. Uncertainty and anguish increased as time went by without a cure for what loomed as the greatest threat since the Black Death of the fourteenth century. Meanwhile, the threat of other pandemics also emerged in the new century, including a new avian or “bird” flu that began to spread in Asia and appeared to resemble the flu virus that had caused the deadly worldwide pandemic of 1918.

Nuclear Physics

In pure, or theoretical, science the transformation of physics in the twentieth century could be compared only to the scientific revolution of the sixteenth and seventeenth centuries. Early in the twentieth century scientists had discovered the natural radioactivity of certain elements, physicists like Max Planck and Albert Einstein had developed quantum physics and relativity theory, and Einstein had propounded his now famous formula for the conversion of mass into energy (e = mc2). After 1919 a series of discoveries led to a deeper understanding of the atom. The cyclotron, developed by British scientists in 1932 at Cambridge University, made it possible to penetrate or “bombard” the nucleus of the atom at high speed. The nucleus, scientists learned, consisted not only of protons but of other particles like neutrons as well. In 1938 the German chemist Otto Hahn discovered that when he bombarded the atomic nucleus of the heavy radioactive element uranium with neutrons, it became unstable and split into two, which meant that energy trapped within the atom could be released. Additional scientists, including Lise Meitner, a German-Jewish scientist who had been forced to flee Nazi Germany, helped to expand upon these findings. In 1939 Nils Bohr, a Danish physicist, brought word of these developments to American scientists.

The implications of this breakthrough in theoretical science were clear. If the atoms in a large amount of uranium were split in a chain reaction, enormous amounts of energy would be released. In the troubled atmosphere of 1939 the possibility arose of its use for military purposes. When the war came, scientists in the United States, including Einstein, who had fled the Nazis in 1934, prevailed upon the American government to explore its military use before the Germans succeeded in doing so. In 1942 American and British scientists and European refugee scientists like the Italian Enrico Fermi brought about the first sustained nuclear chain reaction. This in turn led to the secret preparation of the atomic bomb at Los Alamos, New Mexico and to its use by the United States against Japan at Hiroshima and Nagasaki in August 1945.

After the war even more staggering technical developments followed. The hydrogen or thermonuclear bomb was built independently by the Americans and by the Soviets in 1952-1953; it involved nuclear fusion, or the joining together of hydrogen and other elements at great heat, using the atomic or fission bomb as a detonator with a stupendous chain reaction. Such thermonuclear fusion is responsible for the energy of the sun itself.

The first use of nuclear energy was therefore for military purposes, but it held constructive peacetime potential; a tiny grain of uranium (or plutonium, another radioactive element) could produce power equal to almost three tons of coal. By the 1990s over 15 percent of the world’s electricity was generated by nuclear power plants; in France, over 65 percent. At the same time the awesome explosive powers unleashed by the scientists and the dangers of radioactivity were alarming. Accidents in nuclear power plants threatened the surrounding population and environs with the release of radioactive gases; nor could nuclear meltdowns be ruled out. Popular opposition to the building of nuclear power plants grew, as did concern over their proper design and the disposition of nuclear waste.

In military affairs a true nuclear clash of arms could mean apocalypse. Scientists such as Robert Oppenheimer who had helped to develop the first atomic bomb believed it to be so awesome a weapon as to make its future use unthinkable and in that way bar future major wars. In part they were correct; a balance of terror emerged so that over the years since August 1945 no nuclear weapon has been used in warfare. But nuclear bombs became part of the world’s arsenal, an ever-present threat to human life and to the planet itself.

In later years scientists used linear accelerators like the cyclotron and even more powerful colliders and supercolliders as atom smashers to explore the nature of the atom and the behavior of its subatomic particles. Theoretical physicists continued to advance complex new concepts such as string theory and thus persisted in their search for an overarching theory that would explain the interrelationship of gravity, electromagnetism, and nuclear force, all of which were to be found in the subatomic world and in the cosmos as a whole.

Social Implications of Science and Technology

As in the case of nuclear physics, science in the contemporary age was closely allied with technology and the organized effort to exploit new scientific findings. Government and industry subsidized most scientific research. Laboratory equipment was expensive, and complex investigation required large-scale collaborative efforts; the solitary scientific investigator virtually disappeared. The subsidization of research for national purposes raised fears that scientific discoveries might serve political goals rather than meet pressing social or human needs.

Science had always affected the way people thought about themselves and their universe. The Copemican revolution had removed the earth from its central position in the scheme of things; Darwinian evolution had demonstrated that Homo sapiens was biologically a species that had evolved and survived. The philosophical implications of contemporary physics were only vaguely understood, yet they reinforced theories of relativism in all spheres. Ironically, at the very time that the average person was awed by the capabilities of science, scientists themselves recognized that they did not possess a magic key to the nature of things. Generally they claimed no more than the ability to determine, or guess at, relationships, which in the world of the atom (as in the cosmos itself) remained mysterious and uncertain.

Some thoughtful persons questioned scientific and technological advance and asked whether modern technology had grown beyond human control. Ecologists pointed to the wastage and despoliation of natural resources and the threat to the environment. In another way the lifepreserving features of medicine and public health could result in overpopulation and in perhaps unmanageable pressure upon the limited resources of the globe. The techniques developed to save or prolong human life also raised ethical and legal issues, including new definitions of life and death, and the rights in such matters of patients, families, hospitals, and physicians. Questions arose over test-tube fertilization and other forms of artificial conception made possible in the late 1970s, and later over stem-cell research using human embryos. Those who condemned modern technology extolled the virtues of a prescientific and preindustrial age; others called for sharper awareness of how scientific advances posed new dangers or new ethical dilemmas. In an age of destructive weaponry and threats to the environment, no longer was the advance of science and technology unequivocally equated with the idea of progress.

Meanwhile, in the quest to understand nature the old divisions between the sciences broke down and new sciences appeared. Biochemistry, cell and molecular biology, biophysics, astrophysics, geophysics, and other subdisciplines arose; and all made intensive use of mathematics. In biology genetics made striking advances. While physicists discovered new atomic particles, biochemists isolated the organic substance found in the genes of all living cells, the chemical carriers of all hereditary characteristics. When scientists deciphered the genetic “code” and synthesized the basic substance of heredity (DNA), it became possible by splicing genes to alter the characteristics of plant and animal species and to clone or reproduce in the laboratory an animal such as a frog or a sheep with desired hereditary characteristics. The implications for the possibility of even human cloning at some future date were staggering. Meanwhile biotechnologists were revolutionizing food production by genetically engineering new varieties of crops, which provoked strong objections in various quarters. Medical scientists could also hope to conquer inherited diseases, a goal that moved closer to reality when researchers announced in 2000 that they had decoded the human genome—the more than 3 billion chemical units of DNA that control the human body and also contribute to genetic-based illnesses.

The other life sciences and social sciences also grew in importance. Psychological exploration of human behavior, as well as the treatment of mental and emotional disorders through psychiatry and psychoanalysis, expanded rapidly. Freud, who had first developed his theories of psychoanalysis before 1914, became more widely known in the 1920s. His emphasis on the human sex drive and sexual repression was later much modified. Many students of human behavior argued that his contributions were not universally or scientifically valid but reflected the values of pre-1914, middle-class, male-dominated Viennese society. A variety of schools emerged with different interpretations and techniques, and drugs were developed to treat psychological depression and other mental illnesses. But the search, initiated by Freud, for the unconscious sources of individual and collective human conduct remained a hallmark of contemporary thought and culture.

Anthropologists and other social scientists increasingly stressed the relativism of all culture. They denied notions of cultural superiority or hierarchies of cultural values, or even that there were objective criteria of historical progress. If Western society, they noted, made notable progress in science and technology, other cultures accomplished more in self-discipline, individual integrity, and human happiness. The adjective “primitive,” as opposed to “civilized,” tended to disappear, and a cultural humanism emerged that recognized and esteemed values distinct from the Western tradition.

Space Exploration

Among the most dramatic developments in science and technology in the second half of the twentieth century was space exploration. In the 1950s the Soviets and the Americans competed with each other as part of the Cold War; each made important advances in rocket research. Both the U.S. and the U.S.S.R. mastered multistage rocket launching. The Soviets opened the space age when in 1957 they launched Sputnik, the world’s first artificial satellite; in 1961 they sent the first human, Yuri A. Gagarin, in orbital flight around the earth. The Americans sent their own astronauts into space in 1961 and 1962. In the 1960s both countries launched unmanned automated spaceships to probe and explore the moon and then the planets of the solar system and their satellites. Early in the 1960s President Kennedy pledged that Americans would land on the moon before the end of the decade. In 1969 three American astronauts, as planned in Project Apollo, made the quarter-million-mile journey to the moon, and millions of viewers around the globe watched on television as Neil Armstrong took his first steps on the moon. The Americans made five additional trips to the moon over the next three years. Automated robot spaceships were launched in the 1960s and later on long journeys to explore the planets of the solar system. The American spaceships approached or orbited the planets and soft-landed instrument packages, which over a period of years sent back a rich harvest of data, providing more knowledge than ever before possessed about the nature and origins of the planets and the earth itself. The later space probes explored the more distant planets. Voyager 2, launched in 1977, traveling close to 4.5 billion miles over 12 years’ time, reached the outermost planets before flying into interstellar space. In 1990 the United States placed in orbit the Hubble space telescope, capable of seeing light years away into distant galaxies and reporting back its findings.

The Soviets also conducted impressive space probes to the moon, to Mars, and to Halley’s comet, whose behavior had been predicted in the seventeenth century and which once again returned close enough for observation in 1986. The Soviets also built a permanent space station and set records in testing human endurance in weightless space.

As time went on, Cold War rivalries played less of a role in space exploration, but military objectives were never completely absent. The U.S. and U.S.S.R., and later other countries, launched spy satellites for reconnaissance and information gathering. The United States initiated research in the 1980s into an enormously costly strategic defense initiative that would deploy laserlike weapons in outer space (hence called “Star Wars”) as a defensive shield against nuclear missile attacks. The project was abandoned in 1990 but was revived again in the following decade, with ardent champions and equally vehement critics.

In the 1980s space enterprise showed promise of international cooperation and was no longer confined to the military superpowers. France, Japan, China, and other countries became spacefaring nations, planning and launching satellites and space probes of their own. The European Space Agency carried out its own operations. A multinational human expedition to Mars sometime in the twenty-first century was discussed. After the Cold War the Americans and the Russians began to cooperate in space, most notably in the development and joint use of a Russian-built space station. On the other hand, the need for human expeditions came under serious scrutiny. Dismayed by the human cost of deadly accidents (including the disintegration of the American space shuttle Columbia as it reentered the earth’s atmosphere after a flight in 2003), some critics contended that unmanned space flights might accomplish more, at less cost, and at less risk to human life. Many objected to the enormous expense of space exploration when acute social needs at home remained unfulfilled. But its champions defended it as part of the continuing human effort to cross new frontiers, expand horizons, and explore the unknown. The twentieth century may ultimately be remembered, apart from the destructiveness of its terrible wars, as the century in which humans first set foot on the moon, and with robot spaceships devised and guided by human intelligence, began to explore the universe.

Philosophy: Existentialism in the Postwar Years

In the early years after the Second World War, a new group of philosophers in Europe and North America developed a loosely organized body of ideas called “existentialism.” The existentialists formed no one school of thought and held no coherent body of principles. There were religious and atheist existentialists. Yet all held some beliefs and attitudes in common. All reflected a troubled civilization, a world disturbed by war and oppression, a civilization of material progress and moral uncertainty in which the individual could be crushed by the very triumphs of science and technology, and in which human life itself seemed to have lost higher purpose or inherent meaning.

Existentialist thought owed a debt to Pascal, Nietzsche, and others who had underscored the tragic element in human existence and the limitations on the power of human reason. More directly it owed a debt to Søren Kierkegaard, the nineteenth-century Danish religious philosopher. But it was French writers, and especially Jean-Paul Sartre, who after the Second World War developed existentialist thought in literature and philosophy in a form that for a time gave it a wide popular following. In a hostile world, the existentialists contended, human beings had to make choices and commitments on their own. They were “condemned to be free” and were alone responsible for the choices and actions that defined their very existence; for most existentialists, the nature of a human being’s existence was defined by what he or she did rather than by some deeper spiritual essence. Authentic existentialists therefore had to move beyond philosophical contemplation and take action in the world, even though they were aware that human action might fail to change the world. Albert Camus, influenced by the existentialists, drew upon the myth of Sisyphus, who was condemned continuously to roll his stone uphill, though it always rolled back down again. The very humanity of Sisyphus grew out of courage and perseverance at a hopeless and absurd task. Existentialism emphasized the anguish of human existence, the frailty of human reason, the fragility of human institutions, and the need to reassert and redefine human freedom. Although its popular following waned and Sartre himself was dislodged from his earlier lofty eminence, existentialism never completely disappeared from contemporary philosophy or religion.

Philosophy: Logic and Language, Literary Criticism/ History

Professional philosophy in the twentieth century seemed to contribute less to an understanding of contemporary problems than in the past. Always concerned with the origins and nature of knowledge, it had also shared an interest in metaphysics and ethics. In the early twentieth century it became highly analytical, focusing especially on the limits and criteria of knowledge. In the formal study of logic, mathematical symbols replaced the use of traditional language. On the eve of the First World War Bertrand Russell and Alfred North Whitehead had explored logic and mathematics in their monumental Principia Mathematica. In the 1920s an influential group of philosophers and mathematicians in Vienna, among them Ludwig Wittgenstein, sought to introduce the methodology and precision of mathematics into the study of philosophy as a whole, in what they called logical positivism. They rejected the ambiguities of language used in traditional speculation on morals and values, turning away from the nondemonstrable—that is, “God, death, what is higher,” in Wittgenstein’s phrase. The Vienna group disintegrated in the 1930s, but logical positivism remained influential. Most professional philosophers continued to emphasize scientific rigor and linguistic analysis. A smaller but growing number, responsive to contemporary ethical concerns, devoted themselves to unresolved human and social dilemmas.

New studies in philosophy, linguistic analysis, and semiotics (that is, the study of signs and symbols in communication) drew attention to the complex relation between language and reality. Philosophers also challenged many of the dualisms taken for granted in Western thought. In literary studies “deconstruction” theory emerged after the 1960s to offer new methods of analysis and criticism. Its proponents sought to analyze, or “deconstruct,” a body of writing or “text” (which could be a painting or other kind of cultural object as well) to examine its implicit cultural assumptions and its indebtedness to its cultural traditions. No single valid meaning was to be attached to any individual work. According to its advocates, deconstruction made it possible to reveal the philosophical, class, racial, ethnocentric, or sexual assumptions hidden in the language of a work. It also contested older hierarchical standards of literary quality, blurred distinctions between elite and popular culture, made less of a dichotomy between fact and fiction (and other dualisms), and broadened the existing canon of writings studied in literature, history, law, religion, and other disciplines.

The work of the critic was said to be as much a creative enterprise as literary or artistic creation itself. Propounded originally in the late 1960s in broad philosophical terms by the French philosopher Jacques Derrida, and developed further by other writers and scholars of various disciplines and nationalities, deconstruction won its largest following in the United States. Its opponents viewed it as abandoning traditional literary history and rejecting rational, critical standards that had shaped modern thought since the Enlightenment.

The writing of history also underwent a profound change. A group of French historical scholars (called the Annales school, from a journal with which they were associated) gained wide influence after the Second World War. They focused on long-term elements in historical change such as population, economy, climate, and natural resources; relegated politics to a lesser role; and avoided the traditional narrative of “events.” They studied also the lives of ordinary people in the past and tried to reconstruct the collective outlook of social classes. The newer social history in France, England, and the United States also paid special attention to the inarticulate and illiterate and to all those with strong oral traditions, such as the American slaves in the antebellum South, the English working classes, and the peoples of Africa, reconstructing their culture and lives despite scarce written sources. Another new emphasis greatly expanded the historical study of women from antiquity to the present, leading, among other results, to cultural and social reassessments of entire historical eras. A variety of social themes also received new historical attention: marriage, divorce, the family, childhood, sexuality, even insanity and death through the ages. For their part, many traditional historians who had not been wholly insensitive to these concerns widened their own narratives to include such social and cultural themes.

The Creative Arts

The revolution against older traditions in the creative arts assumed new dimensions. Ever since the Renaissance visual artists had followed certain norms of representation and space perspective. But much of twentieth-century art prided itself on being nonobjective; it rejected the idea of imitating or reconstructing nature, or mirroring it with realism or photographic fidelity. The artistic revolution inaugurated in France before 1914 accelerated in the interwar years and after 1945. It seemed to mirror the political turbulence of the times and the disillusionment with rationalism and optimism. It reflected the influence of Freudian and other schools of psychology and the emphasis on the unconscious and irrational, as well as the relativity of the new physics and its uncertainties about the nature of matter, space, and time.

Artists in the contemporary era continued the pre-1914 experimentation in color, form, and use of materials, but went well beyond, seeing the world around them in new ways. The innovative work of earlier artists such as Picasso—whose cubist paintings had systematically distorted and deformed material objects or human figures—was followed by increasingly abstract artistic experiments during the later twentieth century, as may be seen by comparing Picasso’s Les Demoiselles d’Avignon on p. 619 with Matta Echaurren’s Invasion of the Night at the top of this page. Some artists expressed themselves through geometric form; others left reality behind and tried to represent their own unconscious fears or

desires. The results were fascinating but frequently baffling, as in the painting by Jackson Pollock, Number 1,1950, Lavender Mist, which appears above. After the Second World War, the United States took over from Europe the leadership role in the visual arts and evolved new forms of abstract art.

Contemporary art resulted in original and striking expressions of form and color, but the conscious subjectivism widened still further the gap between artist and public. The artist, painter, and sculptor (and the poet, musician, playwright, and novelist, who were also rejecting the older conventions) were conveying their own visions of the world, not an objective reality that could easily be understood by others. Perhaps the greatest innovation was that the public, baffled as it was by much of contemporary art, came to accept the avant-garde as normal, even if on occasion it rebelled against it. Democratic societies accepted the need for artistic experimentation and innovation, which had been frowned upon or banned as degenerate or socially dangerous in totalitarian societies like Nazi Germany and the Soviet Union. Representational art, of course, never completely disappeared anywhere, and many artists reaffirmed it, contributing to a growing pluralism in contemporary art.

The focus on subjectivism and the unconscious was reflected in literature too. The reconstruction of lost time and the unfolding of the individual’s innermost experience through a stream of consciousness and flood of memories, which had appeared first in the work of Marcel Proust and James Joyce before and shortly after the First World War, remained important for a new generation of novelists and playwrights after 1945. Not only writers but also cinematographers experimented with probing the unconscious in evocative but mystifying ways. All of this cultural experimentation contrasted with the popular entertainment provided through the mass media, especially movies and daily diets of television “soap operas” and “sit-coms.”

Sometime in the early 1970s the phenomenon of postmodernism emerged in architecture, literature, and other art forms. In all areas the postmodernists borrowed from the past and mixed the old and new, and the popular and elite, to suit their tastes. Unlike modernists from the late nineteenth century on, the postmodernists did not reject the commercialization and materialism of contemporary culture but embraced it and incorporated it in new ways, often with humor. The American architect Robert Venturi and his collaborators wrote a book called Learning from Las Vegas (1972). Composers introduced street noises (and silences) into their music. Repetition, as in the commercial world of packaging, marketing, and television advertising, was adopted as an artistic technique. The artist Andy Warhol painted serial pictures of Coca-Cola bottles and of the film star Marilyn Monroe. In fiction the postmoderns mingled actual events and fantasy. A play by Harold Pinter, winner of the Nobel Prize for literature in 2005, Samuel Beckett, or Eugene Ionesco challenged traditional theatrical conventions. Indeed, a Pinter play such as The Homecoming or a Beckett play such as Waiting for Godot could be as baffling as a work of postmodernist art; and it was very different from a play by Shakespeare, Molière, Ibsen, or Shaw. The postmoderns also rejected traditional ideas of structure, seeing no need in literature or art for a beginning, a middle, and an end. Postmodernism was both a phase of the modernist rebellion against traditionalism and a fragmented sequel to it with new messages. Where modernism typically sought to convey an artist’s unique personal vision, postmodernists insisted that writers inevitably expressed the language and values of their culture rather than a distinctive individual consciousness. Because the writer’s life and personal experience were now deemed less important for the study of literature, it was possible to speak metaphorically of the “death of the author.”

Religion in the Modern World

Religion was in flux in the modern world. With the continuing inroads of secularism, the challenges of science, and the post-1945 advances of communism in eastern Europe, China, and elsewhere, organized religion encountered many obstacles. But the churches survived Marxist regimes and retained their vitality. Statistics on religious affiliation are never exact, but some figures on the religions with the largest number of adherents in the opening years of the twenty-first century were clear. Islam, with well over 1 billion adherents, was the fastest-growing faith; it made large inroads in postcolonial Africa. There were close to 800 million Hindus and 350 million Buddhists in the world. Christianity, if aH branches are added together, remained the largest religion, counting close to 2 billion adherents, of which more than half were Roman Catholic, a fourth were Protestant, and a fourth were Eastern Orthodox.

The ecumenical movement in Christianity, that is, the organized effort to unite the many branches of Protestantism, and eventually all Christianity, which began in the nineteenth century, made headway throughout the half-century after the Second World War. A World Council of Churches was founded in 1948. In historic breakthroughs toward the end of the century Lutheran churches announced a reconciliation and alliance with the Episcopalian churches, and Lutherans and Calvinists resolved theological differences over Luther’s definition of salvation, “justification by faith.” When in the 1960s the Roman Catholic church abandoned its insistence on a privileged position within Christianity, it too encouraged ecumenicism. All Christian churches moved toward a closer dialogue with non-Christian world faiths as well.

As in the late nineteenth century, tensions between modernism (in its religious sense) and fundamentalism continued. Many of the Protestant churches in the twentieth century for the most part reconciled their traditional teachings with science and Biblical scholarship, minimized the supernatural and dogmatic aspects of their faith, and sought to adapt the teachings of the gospel to the social needs of the contemporary world. But the two world wars and other social and cultural upheavals dealt a blow to theological modernism and to the inherent optimism of the social gospel. In the 1920s, not only did fundamentalism revive but an intellectual reaction also set in among Protestant theologians who emphasized revealed religion and elements of faith. The Swiss theologian Karl Barth in his writings from 1919 to the 1960s endeavored to lead Protestantism back to the root principles of the Reformation. There was much interest in Kierkegaard who, like Luther, had resolved his own deep anguish by a personal commitment to religious experience. After the Second World War, as a result of the work of Barth, Paul Tillich, and others, a powerful movement in Protestantism reasserted its dependence on revealed religion and denied that human reason could ever properly judge divine revelation. Some church writers, unable to explain the wrenching experience of the Second World War and the Holocaust, spoke of “post-Auschwitz theology” and of “God’s removal from history.” Evangelical Protestantism, with its literal adherence to the gospel, spellbinding preachers, and revivalist emotional appeal also flourished, especially in the United States, and conducted a continuing crusade against the teaching of Darwin and evolution in the schools. But other Protestants generally accepted religious scholarship and the intellectual validity of modern scientific knowledge.

The Roman Catholic church was passing through one of its great historic phases beginning in the second half of the century. Although the church no longer actively suppressed all forms of modernism, the Vatican in the early postwar years reaffirmed dogmatic training in the seminaries. In 1950 Pius XII (1939-1958), who headed the church during the Second World War (and was criticized in some quarters for insufficiently opposing the terrible Nazi atrocities against the Jews), proclaimed the Assumption, the literal, or bodily ascent of the Virgin Mary into heaven, the only new Roman Catholic dogma to be promulgated in the entire twentieth century.

Pius XII was succeeded in 1959 by John XXIII. Although elected at the age of 77, and reigning for only four years until his death in 1963, John proved to be one of the most innovative popes of modern times, working to bring the church and its teachings into greater harmony with the contemporary world. His powerful encyclicals gave a global emphasis to the older social teachings of the church and called upon the wealthier nations to share their resources with the less favored. The first encyclical ever addressed to Catholics and non-Catholics alike, Pacem in Terris (1963), appealed for peace and human rights. A champion of ecumenicism, he opened dialogues with other faiths. In 1962, against the advice of his own theologians, he convened the Second Vatican Council, the first such council since 1870, and as it turned out, the most important since the sixteenth-century Council of Trent. Vatican n, as it came to be called, reshaped contemporary Catholicism.

John did not live to see the Council’s labors completed, but was the principal inspiration for its reforms. His successor Paul VI (1963-1978) shared John’s social concerns and encouraged ecumenicism, but was more conservative in other ways. The Council completed its work in 1965. Accepting the principle of religious pluralism, it abandoned the older insistence on a Catholic monopoly on religious truth. It affirmed the principle of collegiality, which had gone into eclipse in the modern centuries, the view that the pope must share his authority with the prelates of the church, thus strengthening the authority of the national churches on substantive matters. It revised the liturgy and various church practices. The Mass, henceforth, would be conducted in vernacular tongues instead of in Latin, which had been the rule for centuries. The Council relaxed restrictions on dress for priests and nuns. In one historic declaration, the Council explicitly absolved the Jewish people from the charge of deicide that had fed and inflamed anti-Semitism over the ages. John XXIII’s goal—the revitalization and updating of church teachings and practices—was amply fulfilled by the Council. There were limits to the changes, to be sure. The Council reaffirmed celibacy for the clergy and refused to sanction the ordination of women as priests. Paul VI, meanwhile, upheld papal supremacy and took a firm conservative stance on moral issues, especially against all artificial means of birth control.

After Paul VI’s death in 1978 (and when a successor, John Paul I, died after only 34 days in office), John Paul II, the archbishop of Cracow, became the new head of the church, the first Polish pope ever elected, and the first non-Italian pontiff in over 450 years. Robust, earthy, energetic, versatile in languages, and with a keen sense of pageantry and papal majesty, he brought an added dynamism to the church. Not only did he encourage the Christian ecumenical movement, but he also reached out to non-Christians as well, traveling widely in Asia, Africa, and Latin America. During the Cold War years he entered into diplomatic negotiations with the Soviet Union and the Communist countries of eastern Europe to improve the status of the church and contributed much to the revolutionary transformation of his own country and eastern Europe in the 1980s.

In his indefatigable global travels John Paul presided over Mass for millions of Roman Catholics, and he offered apologies for past wrongs going back to the Crusades and including the abuses of the Inquisition, whose archives he opened to historians. He apologized for the Inquisition’s condemnation of scientists like Galileo. In formal statements and documents he offered specific apologies to the Jews for abuses they had suffered over the centuries and especially for the ordeal of the Holocaust, and in a historic visit to Jerusalem he prayed at the Western Wall.

Although on global, social, and economic issues John Paul held progressive views and was sharply critical of unbridled capitalism and irresponsible materialism, in matters of church doctrine and governance he favored orthodoxy and papal supremacy. He appointed conservative archbishops, bishops, and cardinals, silenced dissenting theologians, and curbed the growing assertiveness of national churches. He would not countenance marriage for the clergy, the ordination of women as priests (an innovation accepted by the Church of England in 1994), rights of divorce (or of remarriage for the divorced), or homosexuality.

John Paul’s stance on many issues engendered protest against the “new Roman centralism,” the failure to modernize the church more thoroughly, and the unwillingness to respect the spirit of shared authority promised by the Council. His defenders argued that in upholding tradition he was restoring a balance upset by overly rapid changes in the church introduced by Vatican II. Although weakened by declining health in his later years, John Paul remained active in setting church policies and in defending his theological principles until the last days of his life in 2005. The church cardinals quickly chose a 78-year-old German theologian who had been a close aide of John Paul to serve as the next pope. Entering office as a well-known spokesman for his predecessor’s policies, the new pope, Benedict XVI, undertook to defend the ideas and administrative system that the Vatican had promoted during the long and momentous pontificate of John Paul II.

Judaism was haunted in the years after 1945 by the traumatic experience of the Holocaust. At the beginning of the twenty-first century there were about 14 million Jews in the world, including 5.8 million in the United States, 4.6 million in Israel, and 2.4 million in Europe. The earlier trend to secularism persisted, but more striking was the vitality of all its branches, Orthodox, Conservative, and Reform. Jews everywhere, and especially in the United States, many not Zionists, lent moral and financial support to the state of Israel, although many were troubled by Israeli militancy and intransigence and by the role in domestic politics of ultraorthodox religious groups that refused to accept a secular state. Anti-Zionism, not only in the Arab world but elsewhere as well, at times served as a thin screen for anti-Semitism. In the former Soviet Union (as in several Islamic states in the Middle East), Jews had met harassment and persecution and, when permitted, emigrated in large numbers. The collapse of communism reignited older currents of anti-Semitism in eastern Europe.

The major non-Western religions, Islam, Hinduism, and Buddhism, also made efforts to adjust millennia-old doctrines to the secular tendencies of the contemporary age; but in some instances such changes were adamantly rejected. Eastern religions attracted new adherents in the West: Islam drew many new believers in the multicultural cities of Europe and North America; Buddhist meditative teachings and practices gained new followings among Western students, intellectuals, and others. The large numbers of African and Asian immigrants to western Europe, and of Asian and Hispanic immigrants to the United States, brought their religious faiths with them, transforming both religious and cultural life in the increasingly diverse nations of the modern world.

The latter part of the twentieth century also saw the rise and spread of militant religious reform movements (often called “fundamentalist” by outsiders), especially in the Islamic world. All such movements rejected modern secularism and turned to the literal reading of ancient texts for their own guidance and to impose rules of conduct on others. Nowhere did popular frenzy and the adulation of a religious leader reach such heights as in Iran in 1979; but Muslim activists were also at work in Egypt, Algeria, Nigeria, and elsewhere seeking to subvert secular regimes, impose theocracies, and use state power to enforce religious views. Iran after its revolution in 1979 and Afghanistan under the Taliban regime were among the most extreme examples of countries where Islamic law, or sharia, took precedence over all other law. In several states, as in Sudan, militant regimes also attempted to impose Islamic law on nonbelievers.

The fundamentalists in all societies typically demanded an unswerving adherence to the sacred texts regardless of changing times and circumstances; an identification of religion with political and moral values; and a strict moral code extending to diet, dress, and the relationship of the sexes—all interpreted and enforced by messianic religious leaders who mobilized mass followings to accomplish these ends. Muslim fundamentalism had a counterpart in an extremist Hindu movement in India, which unleashed violence against the Muslims in the country and threatened the secular state.

Fundamentalism could be found in evangelical Christian sects and in extremist Orthodox Jewish quarters as well. It easily bred intolerance and separatism, and ran counter to the blending of cultures in the contemporary world. In the West the separation of political and religious authority had become so widely accepted that it often became difficult to understand the strength and appeal of the new religious ferment.

Activism: The Youth Rebellion of the i 960s

In the second half of the twentieth century young people acquired a collective cultural and generational identity that young people seemed not to have possessed in earlier historical periods. It was now possible to speak of a youth market, a youth culture, youth movements. In part the phenomenon was demographic in origin, the result of the extraordinary number of births in the decade and a half after the Second World War—the “baby boom.” A large cohort grew up in a rapidly changing world, and they developed a generational identity through a new popular culture that appeared in music, fashion, films, and advertising. Youth culture became a key component of modern global culture.

In the 1960s a youth political activism made a startling appearance, marked by a widespread student rebellion. A generation came to college age and attended institutions of higher learning in larger numbers than ever before. It was a generation that grew up in an era of global change and scientific breakthroughs, to which their elders were not dependable guides. They took for granted the scientific, technological, and other accomplishments of their world and concentrated on its deficiencies—the flagrant contradictions of wealth and poverty within and among nations, racial injustice, discrimination, colonialism, the impersonal quality of mechanized society and bureaucratized institutions (including colleges and universities), the violence that destroyed human beings in continuing wars, and always the threat of nuclear destruction. The rebelliousness extended beyond the traditional generation gap; it was directed at all established society and reiterated romantic or utopian themes that had influenced social criticism since the beginnings of modern history.

The revolt burst forth in the late 1960s in widely separate parts of the world. The cultural revolution in China, although in an entirely different context, was not unrelated and indirectly inspired it. At the peak of the movement in 1967-1968 students demonstrated and rioted on campuses and battled police all over the world; American, Canadian, Mexican, West German, French, Italian, and Spanish universities were heavily involved.

France was one center of the storm. Demonstrations there reached near-revolutionary dimensions in the spring of 1968 and threatened to overthrow the regime when 10 million workers also went out on strike, partly in sympathy with students and partly for their own grievances. But the government eventually restored order, and many of the initial grievances in the overcrowded, impersonal universities were almost forgotten in the wider, but quickly passing, French upheaval.

Some of the earliest and largest political demonstrations took place in the United States, sparked by the African-American struggle for civil rights and heightened by resistance to American involvement in the unpopular Vietnam War. The assassinations in 1968 of the dedicated black leader of the civil rights movement, Martin Luther King, Jr., and of Robert Kennedy, brother of the slain president, fueled further anger in the ranks of the young.

The rebelling students often defined their collective identities through rock music, unconventional styles of dress, and a language of their own. Some activists also made icons out of controversial revolutionary leaders who symbolized opposition to the established Western political order: Fidel Castro and his martyred Lieutenant Che Guevara, Ho Chi Minh, Mao Zedong, militant American black leaders like Malcolm X, the heralds of the colonial revolution such as Frantz Fanon (the West Indian author of The Wretched of the Earth), and others. They read the neo-Marxist philosopher Herbert Marcuse, who warned that the very tolerance of bourgeois society was a trap to prevent true protest against injustice; they learned from him that the industrial working class, co-opted by the existing system, was no longer a revolutionary force. A “New Left” dismissed older revolutionaries in the Soviet Union as stodgy bureaucrats and affirmed that revolutionary leadership would come from Maoist China or other places in the Third World. They attacked materialism, affluence, and conformity, and the power structure of contemporary society. Many believed in militant confrontations that recalled an older anarchism and nihilism. They called on each other to transform or destroy various social, political, and cultural traditions, assuming that their generation could overcome the social hierarchies and injustices in modern democratic societies.

The rebellion in its mass phase faded by the early 1970s. In the United States it helped bring the Vietnam War to an end. Only a small number of extremists carried on a kind of urban guerrilla war through underground terrorist organizations—the Bader-Meinhof gang in Germany, the Red Brigades in Italy, the Weathermen in the United States. Mostly, the rebels of 1968 moved on to places in established society. While many people shuddered at the attack on traditional institutions and orderly processes, others were shaken out of their complacency about social or racial inequities. Efforts were made to reform university administration and to provide more adequate teaching facilities. The youth movement, even after its political radicalism subsided, had a continuing effect on all age groups in loosening older standards of language, dress, and sexual mores.

The Women's Liberation Movement

The feminist, or women’s liberation, movement was another, but more enduring, manifestation of twentieth-century and contemporary social ferment. From the time of the French Revolution, as noted earlier, a few thinkers in France and England had raised the question of equal rights for women, and a women’s political movement had also developed in the United States by the mid-nineteenth century. Elizabeth Cady Stanton and a small group of associates in 1848, inspired in part by revolutionary developments in Europe that year, had proclaimed a declaration of independence for women, demanding the right to vote, equal compensation for work, legal equality in property and other matters, and expanded educational opportunities. In Britain, later in the century, the suffragettes raised similar demands in their militant campaign for the vote. Women won the right to vote before the First World War in a few American states and in a few of the smaller countries; after 1918, in many more nations, including the United States and Britain; and after 1945, in almost all other countries. But other objectives went unrealized.

The militant twentieth-century phase of the women’s movement began in the United States in the mid-1960s, partly as a parallel to the civil rights movement of the African-American population. The women’s liberation movement, inspired by such books as Simone de Beauvoir’s The Second Sex, published in France in 1949, and Betty Friedan’s The Feminine Mystique, which appeared in the United States in 1963, contended that women, half or more of the human race, had always been and continued to be oppressed by a male-dominated society and that women were systematically denied access to positions of authority, leadership, property, and power. Although the more blatant forms of legal discrimination, in such matters as property rights for example, had been or were being removed, twentieth-century feminists demanded an end to all barriers to equal participation in the economy and society. Betty Friedan’s influential book appealed to women to give up traditional lives devoted mainly to the home and family and utilize their education, skills, and abilities in the outside world as well. Large numbers of women entered the job market for the first time in the 1970s. Between 1970 and 1990 the ratio of women to men working in the economy in the United States or actively seeking employment rose from 37 women per 100 men to 62 per 100 men. The new role for women required adjustments in family responsibilities, which were not always easily arranged or accepted.

The campaign for equal rights called upon a different agenda in the poorer, less developed countries of the world, where women generally had to overcome centuries-old patterns of repression, abuse, and disregard of the most elementary human rights. The United Nations from the time of its founding had committed itself to equal political, economic, and educational rights for women. But in many parts of Africa, Asia, and Latin America adult illiteracy rates were still significantly higher for women and were declining more slowly. In 2000 it was estimated that of 110 million children worldwide not attending schools two-thirds were female. Yet in the developing world, where a majority of the world’s women lived, opportunities for women, especially in education, could help accelerate social advances for the entire population.

At UN-sponsored international conferences government delegates and representatives from nongovernmental organizations affirmed the “universal rights of women” and insisted on their right to decide freely on matters relating to all aspects of their lives free from “coercion, discrimination, or violence.” Discrimination, it turned out, was not confined to capitalist societies. Despite Communist verbal promises, the advance to equality, or even to equality of opportunity, in the former Soviet Union and in the People’s Republic of China did not match the official rhetoric.

Some women, more so than in the past (when only a few reigning women sovereigns were able to exercise political power), came to hold positions of the highest authority in their countries in the years after 1945. Among them were Indira Gandhi in India, Golda Meir in Israel, Corazón Aquino in the Philippines, Benazir Bhutto in Pakistan, Margaret Thatcher in Britain, and a growing list of other women presidents or prime ministers in such widely differing countries as Sri Lanka, Portugal, Norway, Iceland, Nicaragua, Ireland, Bangladesh, France, Poland, Turkey, Finland, Canada, Indonesia, Germany, Liberia and Chile. Women were thus entering widely into modern political life, but it was also true, as feminists and others continued to stress, that men still dominated legislative assemblies and governments in general in most countries of the world at the beginning of the twenty-first century.

Meanwhile the development of advanced contraceptive technology, especially of the birth control pill in the early 1960s, and the legalization and greater access to safer abortion procedures in a growing number of modern societies, provided a new biological freedom and autonomy for women. Changing social patterns tolerating sexual freedom and new and more equal forms of relationships between the sexes in marriage also contributed to social liberation. In the first decade of the twenty-first century women were filling a larger share of places in higher education and in professional schools than ever before. In the United States women students made up more than half the number of undergraduate students in colleges and universities and were close to that ratio in law and medical schools. As more women became part of the labor force at all levels, the demand arose not only for equal compensation for equal work, still far from realized, but also for better pay for jobs that were poorly compensated because they had been traditionally filled by women. Although there were disagreements within and outside the women’s liberation movement on the methods and tempo of change, wide agreement existed on the need to utilize fully all of society’s human resources in every part of the globe. If that could be accomplished, it would count among the most memorable of the revolutionary changes of the contemporary era.

132. INTERNATIONAL CONFLICT IN THE TWENTY-FIRST CENTURY

The end of the Cold War and the demise of the U.S.S.R. in 1991 transformed the foundations on which international relations had rested since 1945. The Western campaign to contain communism, for which there had been a broad consensus in American public opinion, was over. The United States was now the sole superpower, the leading economic and military power of the globe. The key question was whether it could handle this power with restraint and with consideration for its allies and other nations. International stability in good part depended on how it conceived of its responsibilities in world affairs and how it shaped its relationships with the UN, its European allies, Russia, China, and the developing world. A new configuration of world affairs began to emerge, but its outlines and the American role were only roughly apparent. Peace and security remained the world’s most pressing problems. Two new challenges appeared: the eruption of conflicts more frequently within nations than between nations and the danger of terrorism, which spread into almost every region of the world.

The twentieth century, heralded in its opening years for its promise of peace and progress, had seen more than 200 million people perish as victims of wars and brutal regimes. Five million deaths resulted directly or indirectly from armed conflict in the last decade alone. It was clear that the end of the Cold War had not brought peace to the world, but the older patterns of warfare between nation-states seemed to be breaking down by the early twenty-first century. The new wars tended to be conflicts between religious or ethnic groups within a national territory or between guerrilla forces and high-tech national armies that sought to suppress them. These new wars could last for years, in part because they were no longer waged by opposing national governments that were able to negotiate a truce or a peace settlement.

Some close analysts of international affairs maintained that the new world order might best be understood in the context of rivalries and conflicts between religion-based civilizations, not between nation-states. There was already some evidence of this. The Russian offensive against Chechnya had anti-Islamic overtones, as did the Serb and Croat attacks on Muslims in Bosnia and the Serb war against the Albanian Muslims in Kosovo. In turn. Orthodox Christian church leaders in Russia and Greece condemned the Western air offensive against Serbia. India, once intended as a haven for a tolerant secularism, was now increasingly Hindu-oriented, and its clashes with Islamic Pakistan over Kashmir and other matters were sharpened by religious tensions. The conflicts between Israel and its Arab neighbors in the Middle East had distinctive religious overtones. The extremist Muslim Taliban rulers in Afghanistan willfully destroyed ancient Buddhist sculptures as offensive to their faith. Chinese communism displayed something of the older Confucian and imperial traditions hostile to the outside world and was intolerant of religious threats to its rule. The wars launched by the United States against the Muslim countries of Afghanistan and Iraq in the early twenty-first century also took on religious meaning because some saw these conflicts as a battle between different religious and cultural traditions and values, or even as a revival of the medieval Crusades.

But nonreligious factors remained preponderant in international affairs, and even in religion-based civilizations there was marked diversity. A common faith in Islam did not prevent Iran and Iraq from their bloody war in 1980-1988 or protect Kuwait from invasion by Iraq in 1990. The world’s religions were far from homogeneous, and all had been touched to some degree by secularism and globalization. There were enduring conflicts between branches of the major religions—Catholics and Protestants in Northern Ireland, for example, and Shiites and Sunnis in the many parts of the Middle East. The two great wars of the twentieth century had originated in conflicts between nations in the Christian West itself.

It was nevertheless important to call attention to the global diversity of traditions, cultures, and religions if only to shed any illusions in the West that it was destined to spread its ideas and values across the world unchallenged. Western civilization, with its Judeo-Christian heritage, had been enriched by interaction with other civilizations. By the twenty-first century its global influence was undeniable, but people in the West needed to recognize the existence of deep-seated cultural allegiances in every part of the world and to understand the enduring importance of cultural differences in diplomatic affairs, economic exchanges, and political conflicts.

The United Nations

The much-used term “international community” was indispensable but defied precise definition. The closest approximation to it was the United Nations. Here the Security Council was the dominant institution. The original roster of five permanent members of the Council, the victors in the Second World War, had been modified only in 1971 when the People’s Republic of China replaced Nationalist China and in 1991 when Russia succeeded to the seat of the dissolved U.S.S.R. The political and economic changes in the world spurred proposals for giving other large powers permanent seats, so that they might exercise international responsibilities commensurate with their economic and military resources. Even the veto power of the five permanent powers came into question.

The United Nations counted over 190 member nations by the first decade of the twenty-first century, and even the smallest new nations were permitted to join. The 80 smallest members of the UN, it turned out, represented less than ten percent of the world’s population.

CHRONOLOGY OF NOTABLE EVENTS, 1949-2004

       Simone de Beauvoir’s publication of The Second Sex helps to launch a new international campaign for women’s rights
1968   Youth movements in many nations press for political and social change
1969   American astronauts land on the surface of the moon
1978   John Paul II becomes Pope—the first Pole ever elected to the papacy 1991 Treaty of Maastricht establishes the new European Union 1994 World Trade Organization is established to mediate international trade 1997 Financial crises disrupt the economies of east and southeast Asia
1997   Labour Party wins elections in Great Britain; Tony Blair becomes Prime Minister
1998   Political agreement in Northern Ireland offers plan to end violence between Protestants and Catholics
1998   Social Democrats win elections in Germany; Gerhard Schroder becomes Chancellor
2001   Al-Qaeda terrorist attacks on New York and Washington; United States and its allies overthrow Taliban regime in Afghanistan
2002   The “euro” becomes the common currency of 12 European nations
2003   American and British forces invade Iraq and overthrow the regime of Saddam Hussein, but face prolonged war against “insurgents”
2004   European Union membership grows to 25 nations

It was in the General Assembly that the new states mainly exercised their authority. Regardless of size, each member deliberated and voted in full equality. As we have seen, the General Assembly frequently served as a forum for the grievances of the developing nations against the wealthier industrial world. Several of the UN’s agencies played significant roles in dealing with population, health, social welfare, and humanitarian issues. After the Cold War, the developing nations somewhat tempered their anti-Westernism, but they continued to demand a high priority for their enormous economic needs, including debt relief.

Debate went on in the United Nations, the European Union, and elsewhere over the interpretation of human rights. The Universal Declaration of Human Rights, adopted by the UN in 1948, had focused on political and civil rights, emphasizing protection from arbitrary arrest, imprisonment, and torture. Pressure later mounted to broaden the largely political definition to include economic rights, the necessity of meeting human needs within each country, and the right to international development assistance for those purposes.

An even more complex issue troubled the United States, the UN, and the broader international community: The developing nations at times contended that even the political and civil rights generally described as universal were really Western and should be modified to fit the culture, history, and religions of other parts of the world. Equality for women or the rights of children, in this interpretation, could have different meanings in countries whose cultural traditions differed from those of the West. But many across the globe maintained that human rights, no matter how difficult to define, of Western origin or not, represented a common core of values that should protect every individual human being against enslavement, violation, or discrimination. Respect for cultural pluralism or local traditions should not cover or excuse any form of human indignity.

The practical implications of such debates were significant. In many Islamic countries there were fewer opportunities for women to attend school, work outside the home, drive cars, or vote. The governing regime in Sudan was still sanctioning slavery and forcibly converting its non-Islamic population to Islam. Polygamy was still widely practiced and sanctioned by some religions and cultures. Some African peoples still practiced female genital mutilation. In many parts of the world young children, boys and girls, were forced to serve in combat by guerrilla military units. To abolish cruel and unusual punishments many nations would have to ban flogging, mutilation, and maiming as punishments. The United States itself had a large prison population and retained capital punishment, the latter now banned in the European Union. There was little desire to meddle in the internal affairs of other peoples, but it was difficult to ignore egregious violations of human rights or to accept the argument that Western values were being imposed on others.

Politically, the explosion of virulent ethnic and religious-based nationalism after the end of the Cold War confronted the UN with new issues of humanitarian assistance and intervention. UN peacekeeping came to include help in the reconstruction of devastated countries and peoples as well as a protective military presence. Supporters of the UN’s expanding role urged advanced planning for troubled areas to prevent the escalation of conflict. They projected rapid deployment forces, as once envisaged, with contingents drawn from the member states. Much depended on the support of the United States, which maintained its leadership role in the Security Council but refused to place any of its own troops in UN-controlled peacekeeping forces, criticized the UN’s large bureaucracy, and demanded reforms in the administration and financial management of UN activities.

Although the UN was often criticized for its inability to prevent wars or act decisively on global social problems, its broad goals remained as formulated in 1945: to control and reduce the scourge of war, advance human rights, promote equality, protect the independence of nations, encourage social progress, raise living standards, and work for peace and security. But the world’s nations, large and small, were not ready to subordinate their national interests or sovereignty to any international organization. Without armed power of its own, the United Nations could not prevent powerful sovereign nations from going to war or stop such wars or civil wars after they began.

The United States, as the sole superpower after 1991, exercised the leading role in international affairs, even if in concert with its European allies and other major powers. At times, however, it took initiatives that made it difficult to distinguish between American unilateralism and international action. The Security Council in 1990 condemned Iraq’s aggression against Kuwait, but it was the United States that assembled a formidable multinational military force, including over a half-million American troops, and in 1991 forced the Iraqi withdrawal from Kuwait.

In 1992, toward the close of the first President Bush’s administration, American troops were used for the first time to reinforce UN peacekeeping forces in Somalia, in northeast Africa, where warring militias had made it impossible to deliver food and other supplies to the starving population. President Clinton in the next administration continued the policy, calling for an “assertive multilateralism.” But peace had not yet been achieved and more than peacekeeping was required. In a failed military mission in October 1993, American military units under American command were attacked by Somali paramilitary groups and 18 American soldiers were killed. Publicly televised scenes of corpses dragged through the streets incited Congress to sweep away “assertive multilateralism” and for a time restrict similar intervention.

Recommendations to enlarge the authority and functions of the UN lost support and even seemed to threaten the peacekeeping role that the UN had successfully pursued for some 40 years in places like Zaire (now Congo), southern Lebanon, Cyprus, Cambodia, Angola, and elsewhere. In part the new attitude explained the failure of the UN or the United States and its allies to act in Bosnia in 1992-1993, when outside intervention might have stopped the terrible atrocities against Bosnia’s Muslim population. The same misgivings were behind the failure to take timely action to prevent the massacre of the Tutsi people in 1994 in Rwanda, in which some 800,000 Tutsi were killed and hundreds of thousands were driven into exile; and there was little concerted action to stop the atrocities sanctioned by the Islamic government in Sudan against the non-Islamic black peoples in Darfur.

In some later crises, however, the international community did take action. As we have noted earlier, the massive American-led air offensive by NATO against the former Yugoslavia in 1999 set a precedent for multinational “humanitarian intervention” within a sovereign state, although no ground troops were used. The UN did not join the war, but played a major role in postwar peacekeeping in Kosovo. International war crimes trials were held for political and military leaders charged with war crimes and crimes against humanity in Rwanda and in the former Yugoslavia. An international force also intervened to protect the people of East Timor after they voted in a UN-sponsored referendum for independence from Indonesia and then faced virtual butchery by Indonesian paramilitary forces.

It was a formidable challenge to decide on the legitimate use of international force against sovereign nations to protect threatened or abused populations. As President Clinton noted, the UN’s mission from the time of its charter was to protect the world from the scourge of war. “If so,” he announced in 2000 to a special planning and celebratory millennium meeting of the United Nations, “we must respect sovereignty and territorial integrity but still find a way to protect people as well as borders.” UN Secretary General Kofi Annan strongly endorsed his plea.

NATO, Russia and the New International Cooperation

Many in the United States favored military intervention when American national interests were involved, and many supported the American role in mediation as pursued by successive administrations in troubled areas, such as the Middle East. The new American president, elected in late 2000, George W. Bush, son of the former President Bush, reassessed the concept of “humanitarian intervention,” but soon initiated other military actions when he decided that American interests were at risk. Meanwhile the Europeans took steps to strengthen the independent military and defense role for the European Union. The French, following the precedent of de Gaulle in the Cold War years, called for recognition of a “multipolar world,” in which the EU would be a more equal partner, and they objected, if only in rhetoric, to the hegemony of the “American hyperpower.”

The role of Russia in the new world order was critical but not yet precisely defined. That it was still a major power and not ready to abandon aspirations to an important role in world affairs was evident in its pro-Serb stance in the Balkans. How to deal with an assertive Russia still armed with thousands of nuclear weapons was a major challenge. NATO, meanwhile, at American initiative, admitted Hungary, Poland, and the Czech Republic as full members in 1999; and seven other countries of formerly Soviet-controlled central and eastern Europe joined in 2004. The Central and East European nations not only wished to be full partners in Europe but also saw their membership in NATO as the best guarantee of their future security. The admission of several states on the very borders of Russia, however, was regarded by many Russians as a provocation. On the other hand, Russia’s president became a member of what was now the Group of 8, the heads of government of the major industrial democracies; and there were regular consultations with NATO through a council established in 2002 to facilitate cooperation on various international issues.

Nuclear disarmament loomed as another key issue in the post-Cold War international order. The collapse of the U.S.S.R. promised to end the “balance of terror" that had resulted from the nuclear buildup of the two superpowers. Despite some initial obstacles, the United States and Russia agreed on the voluntary reduction of nuclear arms. The number of nuclear warheads in the world, of which the largest number were held by the United States and the U.S.S.R., was reduced in the 15 years between the mid-1980s and 2000. Nonetheless the head of the UN’s nuclear monitoring agency reported at the close of 2005 that there were still 27,000 nuclear warheads in various parts of the world, which, he said, could mean “the destruction of entire nations in a matter of minutes.”

The threat of nuclear proliferation persisted, keeping the issue high on the international agenda. By the end of the twentieth century 187 nations had signed the 1968 Non-Proliferation Treaty. At one of the periodic meetings held to review the status of the treaty, in May 2000, the five major nuclear powers—the United States, Russia, Britain, France, and China—in the first such declaration ever made pledged themselves “unequivocally” to the eventual elimination of all nuclear weapons. There was satisfaction and reassurance also that since August 1945 no nuclear bomb had been exploded in warfare. That additional nations such as North Korea and Iran might be developing nuclear arms nevertheless gave cause for continuing concern. A major setback to the progress of nuclear arms control was the refusal of the United States Senate in late 1999 to ratify the Comprehensive Nuclear Test Ban Treaty because the U.S. insisted that the nation’s security and the security of other states required periodic testing of the American nuclear arsenal. Almost all other countries ratified this treaty, which would have prohibited all nuclear weapons testing. In addition, President George W. Bush’s proposals to resume work on a missile defense shield and to pursue the development of smaller bombs called “nuclear earth penetrators” threatened to upset existing arms control policy, which since the Antiballistic Missile Treaty of 1972 had focused on the reduction of offensive weapons and banned the building of defensive ones.

If relations with Russia were important, so also were Western relations with the People’s Republic of China. Some saw engagement with Communist China through trade and other forms of peaceful interchange as the path to take. Others insisted that the West could not keep silent in the face of human rights violations, the suppression of religious groups, and the mistreatment of Tibet. The United States was committed to a two-China policy that recognized the continuing existence of a non-Communist Taiwan until peaceful union could take place, but there were alarming Chinese threats of forcible annexation. China could be expected to make its considerable weight felt in world affairs and to oppose future international intervention for human rights abuses.

Meanwhile the world’s attention turned to the pressing problems of international terrorism and to new wars in Afghanistan and Iraq.

Terrorism and War after September 11, 2001

We have seen how the United States became steadily more involved in the Middle East during the latter part of the twentieth century—in the Iran-Iraq War of the 1980s, in the Persian Gulf War of 1991, and in other episodes; but American involvement entered a new phase of extended military conflict in the new century, when on September 11, 2001, radical Islamic terrorists carried out a surprise attack against the United States. Dispatched to America by the extremist Islamic organization known as al-Qaeda, committed terrorists from the Middle East set out on a carefully prepared suicide mission. Fifteen of the 19 terrorists were from Saudi Arabia. They had spent considerable time in the United States, where several had even undergone flight training. The terrorists hijacked four large American commercial airliners, each loaded with passengers, and crashed two of the planes into the towers of the World Trade Center in New York; the third plane crashed into the Pentagon in northern Virginia; the fourth crashed to the ground in rural Pennsylvania without reaching its intended target.

The hijackers managed to accomplish their mission on a busy weekday morning, causing complete destruction of the World Trade Center, crowded with people at work, and significant damage to one side of the vast Pentagon complex. There were 3,000 deaths. No one was prepared for such an attack. There had never been a terrorist attack of this kind and on this scale in the history of the United States or indeed in the whole deadly history of terrorism throughout the world. The closest parallel in the American experience was the Japanese attack on Pearl Harbor in 1941, but that had not taken place in the continental United States and was directed against military targets. Many saw the terrorist attack in September 2001 as a turning point in modern American history, in part because it destroyed the long-held assumption that wide oceans protected the United States from foreign enemies.

President George W. Bush received authorization from the American Congress for the use of all necessary resources to mount a counter-attack against enemies who, he said, “hate our freedom of religion, our freedom of speech, our freedom to vote. . . .“The “war on terror,” which began in September 2001, was defined in terms that in some ways evoked the language of the Cold War against the Soviet Union. Under new legislation called the “Patriot Act,” the president also acquired wide executive powers to deal with terrorist threats within the United States. There was, however, no national enemy that loomed like the Soviet Union as the obvious military opponent in a new global conflict. American intelligence agencies had been tracking the al-Qaeda organization, and there was some later evidence that an intimation of such an impending attack had become available, but little of this was known to the public at the time.

The al-Qaeda leader, Osama bin Laden, had found welcome support from the radical Islamic Taliban regime in Afghanistan, yet he controlled no government or armies or industrial infrastructure that might be attacked in conventional military operations. From time to time he delivered fiery speeches about his self-appointed mission. The al-Qaeda organization had emerged in the aftermath of the first Gulf War against Iraq, when American forces came to be permanently stationed in Saudi Arabia and when the Saudi monarchy entered into a new close alliance with the United States. The “founding statement” of al-Qaeda accused the United States of “occupying the lands of Islam in the holiest of places, the Arabian peninsula, plundering its riches, dictating to its rulers, humiliating its people, . . . and turning its bases . . . into a spearhead through which to fight the neighboring Muslim peoples.” Al-Qaeda called on its followers to launch attacks against America, which had begun in the 1990s with the bombing of American embassies in Africa, continued with a destructive assault on an American naval ship near the Arabian peninsula in 2000, and then escalated in the devastating attack of September 11, 2001.

American forces, with the backing of a strong coalition of NATO allies, at once mounted a large-scale air assault on Afghanistan and, with the aid of Afghan forces opposed to the Taliban, quickly brought about the overthrow of the regime in Kabul. A large number of al-Qaeda militants were captured or killed, but bin Laden himself escaped and remained at large years after the campaign ended. A new Afghan government under President Hamid Karzai, with the help of American and other financial aid, began to rebuild the country, while a continuing but more limited American war against guerrilla forces continued in the mountainous border areas. There was reason to believe that a stable regime in Kabul, together with the support of the neighboring government in Pakistan under President Musharraf, could block the further operations of Islamic radicals and the still active guerrilla forces in Afghanistan.

Although the attempt to capture bin Laden and permanently crush al-Qaeda had proved unsuccessful in the Afghan campaign, the Bush administration decided that the dictatorial Iraqi regime of Saddam Hussein posed an equally serious threat to the United States. Iraq was an oil-rich nation of strategic importance for the entire Middle East, and it now stood accused of stockpiling chemical, biological, and nuclear weapons of mass destruction. The Bush administration had become persuaded that the United States had a mission after the end of the Cold War to curb dictatorships and help spread democracy, even unilaterally if necessary; and it seemed to have harbored for some time a desire to overthrow Saddam Hussein’s regime in Iraq. The war on terror was thus expanded to include plans for a preemptive assault on Iraq, although no imminent threat from that country seemed to be present. To be sure, American and British intelligence reports indicated that Iraq was developing weapons of mass destruction, but other sources were less confident. The Bush administration also linked Iraq to the threats from Islamic terrorism, but there seemed to be no hard evidence of such links. Although some critics decried the impending action against Iraq as a diversion from the true war on terror, the American Congress in October 2002 authorized President Bush to use all necessary force to defend the United States and to compel Iraqi compliance with weapons inspections.

Responding to American concerns, the UN Security Council passed a resolution demanding that teams of UN-appointed weapons inspectors be allowed to enter Iraq and search for the weapons of mass destruction that Saddam Hussein was allegedly hiding. The inspectors were unable to find such weapons, but the American and British governments insisted that the Iraqis were concealing evidence and that the UN should sanction military action to enforce compliance with UN resolutions that Iraq had apparently refused to accept. Britain was the only permanent member of the Security Council to support the United States. When France, Germany, and Russia refused to support the resolution that would have authorized an invasion of Iraq, President Bush and Prime Minister Blair resolved to go to war without the authorization of a UN resolution. A small group of other nations also agreed to provide limited assistance so that it became possible to speak of a coalition force.

The American invasion began in March 2003. Powerful American forces, with the support of Britain and a few small contingents from the coalition, quickly swept north from bases in Kuwait to overthrow Saddam Hussein’s government in Baghdad. Armed resistance melted away, and Saddam Hussein himself fled. Despite massive looting and a general breakdown of law and order, which the strategists of the invasion had failed to foresee, major combat operations appeared to be over. In May 2003 President Bush announced that the mission had been accomplished, and optimistic American planners expected Iraqi oil revenues to finance much of the nation’s reconstruction. But a violent insurgency soon erupted, especially in the Sunni Arab region to the north and west of Baghdad, where much of the population feared the political ascendancy of Iraq’s Shiite majority. The Americans had not fully taken into account the country’s sharp ethnic and religious divisions, which Saddam had kept under tight control. Under Saddam’s officially secular regime and Baathist party dictatorship, the minority of Sunni Arabs, about one fifth of the nation’s 27 million people, had brutally victimized the Shiites, who comprised about three fifths of the population, as well as the Kurds, who were not Arabs but were Muslims, and made up the remaining fifth.

The insurgency had not been expected. Most American strategists had counted on Iraqis to accept the Americans as liberators, not as invaders or occupiers. Within weeks the Americans appointed a “Coalition Provisional Authority” to manage the occupation. It seemed logical for the Americans and the provisional authority they had set up to disband the Iraqi army and police forces and to prevent former Baathists from assuming any new role in the country. But many of the disaffected Sunnis and former Baathists joined the insurgent forces that now sprang up across Iraq. Although the Sunnis, resenting the loss of their former power, were the most active participants in the insurgency, many Iraqis of all backgrounds seemed arrayed against the invasion by the American and Western “infidels.” The insurgents were reinforced by militant Islamic sympathizers who infiltrated from Syria and other Islamic states. The Kurds meanwhile preserved order in their northern region of the country through their own paramilitary forces.

The growing violence seriously disrupted the plans for reconstructing Iraq’s oil industry and technological infrastructure. Electric power and other necessities became scarce, and living conditions were difficult for most Iraqis. In the meantime, a comprehensive American search of the entire country found no weapons of mass destruction—whose alleged existence had been the principal and urgent rationale for the invasion. American forces responded to the new violence by waging campaigns against insurgents in the countryside and major cities, imprisoning large numbers of suspects, and capturing Saddam Hussein and many of his former officials. They undertook to train Iraqis for service in the country’s new security and police forces, many of whom soon became victims themselves of insurgent violence. American casualties also steadily mounted. Roadside bombs destroyed trucks and tanks, and suicide bombers killed or maimed both American soldiers and thousands of Iraqi civilians.

On the political front the Americans took steps toward creating a more independent Iraqi government. In 2004 the Coalition Provisional Authority was dissolved and political authority was transferred to an interim Iraqi government. Three successive elections in the following year brought Iraqis to the polls in large numbers. The first election, in January 2005, chose a transitional parliament whose main task was to write a new Iraqi constitution, but most Sunni Arabs boycotted the election and thereby lost the opportunity to participate in the drafting of the constitution. The constitution was subsequently adopted in a second, nationwide vote in October, and the Sunni Arabs again for the most part refused to participate.

The new constitution generally satisfied the Shiites and the Kurds because it gave virtual autonomy to a large Shiite region in the south and to the Kurds in the north, each with control over the oil resources and revenues in those regions. The Sunnis were frustrated by this political decentralization and by the growing influence of Shiite religious authorities in the new government. With American prodding, it was agreed that the next parliament would take steps to amend the constitution and submit the changes to another national referendum. When the third Iraqi election was held in December 2005, this time for a parliament of 275 seats elected for a four-year term, there was wide participation by all of the nation’s ethnic and religious groups, including the Sunnis, and by a coalition of secular parties. The major Shiite religious parties won the largest number of seats in the new parliament, the Kurds won the expected majorities in their northern region, and the Sunnis gained some seats by winning in the Sunni-dominated provinces and in parts of Baghdad. Both the Sunni and secular political parties, however, charged their opponents with electoral fraud, and a new wave of violence spread across the country. The political situation thus remained unsettled and deeply affected by sectarian and ethnic conflicts, despite the impressive, courageous turnout of Iraqi voters and the widespread desire for a stable, effective, and more democratic government.

The Bush administration refused to set a timetable for the departure of American troops. As the number of American deaths and casualties mounted into the thousands (and Iraqi deaths and casualties increased into the tens of thousands), support for the war declined in the United States, in Britain, and in much of the rest of the world, where public opinion had never really favored the American invasion from the beginning. Criticism grew even sharper when the elections seemed unlikely to produce the stable democratic political culture that had been anticipated and when the results raised fears that militant Shiites might try to establish an Islamic theocracy on the Iranian model. There was also concern that the American departure, whenever it came, would leave divisive ethnic and religious tensions that could lead to a prolonged civil war.

The war caused additional dismay in the United States and elsewhere when news reports accompanied by shocking photographs revealed that American soldiers had humiliated and tortured Iraqi prisoners. Some of the abuses resembled brutal methods that had been common under the dictatorial regime of Saddam Hussein. Although they may have been encouraged by loosely veiled instructions from their superiors who sought information from suspected insurgents, only a few low-rank soldiers were tried and punished. Large numbers of suspects were also imprisoned and apparently tortured by the new Iraqi police. Amid reports of such practices in Iraq and of abusive interrogation techniques at secret American detention centers in other locations, the United States Senate voted overwhelmingly in 2005 to adopt a resolution banning the use of torture by Americans.

Meanwhile, terrorist attacks by al-Qaeda and its sympathizers continued to kill people around the world—from resort hotels in Indonesia to the trains and subways in Spain and Britain. There was growing anxiety that the United States had entered another Vietnam-style “quagmire,” that the war in Iraq had distracted from the wider war on terrorism, and that military action alone would never eliminate the Islamic terrorist threats. Others worried that the war was creating new hostility for America and the West, driving more disaffected and enraged young Muslims into extremist terrorist camps.

President Bush, who won election to a second term in 2004, stressed that the war on terror was strengthening America and its allies. He pointed to the political progress in Afghanistan and Iraq, where millions had turned out to vote, as evidence of a new democratic culture in the Middle East. Much of the American public, however, became skeptical about the president’s defense of the war and his expansion of presidential powers, which included the wire-tapping of American citizens without following established legal procedures. Yet he had pledged to be a “war president,” and he repeatedly reminded the country that the terrorist threat had not passed.

How events might evolve or what might happen in neighboring countries remained unclear. It was certain, however, that the long-anticipated end to the Cold War had not brought the “end of history” or the end of human conflict. In the opening years of the twenty-first century the world was enmeshed in conflicts that reflected the deep-seated forces of contemporary nationalism, global economic competition, and militant ethnic or cultural identities; and such conflicts gained intensity everywhere from deeply held religious beliefs and long historical memories.

133. SOCIAL CHALLENGES IN THE TWENTY-FIRST CENTURY

The Population Explosion

Of all the social developments in the second half of the twentieth century one of the most spectacular was the growth of the world’s population. From about 1950, as a result of medical discoveries, improved health and sanitation measures, declining infant mortality, and more efficient food production and distribution, death rates declined dramatically while birth rates rose. In India, for example, the death rate in 2000 was half the 1950 figure and its population rose from 350 million in 1950 to over to 1 billion at the beginning of the twenty-first century. Globally, the number of human beings grew so rapidly that demographers spoke of a population explosion. In 1950, when the contemporary takeoff began, world population totaled 2.5 billion; at the end of the century it exceeded 6 billion. Never before had human beings lived through a doubling of the world’s population in their own lifetime.

It took the world millions of years for its human population to reach the quarter-billion mark some 2,000 years ago. Not until about 1650 did the population double to a half-billion. It then doubled in less than two centuries to reach its first billion about 1830. By about 1930, in only one century, it again doubled to reach 2 billion. By 1960, in a little over 30 years, it grew to 3 billion; 14 years later, in 1974, to 4 billion; 13 years later, by 1988, to 5 billion; and in 1999 after only 11 years, it passed the 6 billion mark. The time required to add a billion people to the world’s population, and for the population to double in size, had grown shorter than ever before in history. The annual growth rate for much of this time, close to 2 percent, meant the addition of well over 80 million people annually.

Although predictions were never certain, demographers began to find indications of a slowdown in the rate of growth. Sometime between 1965 and 1970 the global annual growth rate seems to have peaked at about 2 percent and then began to decline, unevenly and irregularly, to about 1.5 percent. Nonetheless, because of the huge population base demographers still pushed well into the future the time and estimated figure at which stabilization might be achieved. Even with a reduced growth rate, the world population figure was projected for 2050 at a range of from 8 to 12.5 billion, with some thought that it could peak at just over 11 billion. A debate continued over the “carrying capacity” of the earth, the maximum number of humans that the earth’s resources could sustain.

The population growth rate after about 1965 was showing signs of decline because of declining birth rates, a pattern manifesting itself not only in the industrial world but in the developing world as well. The population increase in the second half of the twentieth century, it will be recalled, was largely a phenomenon of the developing world in Asia, Africa, and Latin America, where three-fourths of the world lived and where birth rates were highest. By contrast, in North America and in Europe birth rates were already lower. Industrialization, urban life, education, and social pressures for smaller families had begun to reduce birth rates ever since the late nineteenth century. In many industrially developed countries in Western Europe as well as in Japan, Canada, Australia, and elsewhere growth rates were now below the population replacement rate of 2.1 children. Italy and Spain had the lowest rates with an average of 1.2 children per woman of child-bearing age, and Italy was expected to decline from a population of 57 million to 51 million in the next 25 years unless considerable immigration took place. In the United States the population was still growing, in good part because of immigration and immigrant families. Measured against the dramatic rise in global figures and in the developing world, Europe and North America were experiencing a shrinking share of world population.

A new social factor was that after earlier resistance to Western calls for control over family size, the developing countries were now accepting the need to limit population growth. In 1970 women in the less developed countries were having an average of six children, but by the opening of the twenty-first century, only 30 years later, women in these countries were having an average of four children. Whether the decline was due to voluntary control over family size, the availability of family planning counseling, or a greater empowerment of women over their lives and access to education and employment was still under discussion.

Although fertility rates began to decline in the developing countries, population growth still threatened to cancel economic advances. Even with lower birth rates the total number of births annually would not decline sharply because of the large population base. Contraceptive technology, with its many advances, was increasingly accessible but still not available to everyone.

The number of couples in the developing world with access to family planning is judged to have grown from 10 percent to 60 percent in recent decades. But at least 100 million women of child-bearing age, it has been estimated, still lacked access to fertility control. Several of the world’s major religions continued to forbid artificial birth control measures, numerous societies restricted educational and vocational opportunities for women, and traditional patterns of life in many parts of the world, especially the poorest, encouraged large families for the sustenance and support of the elderly. The impact of the global population explosion in the second half of the twentieth century would be felt far into the future.

The Environment

As serious a threat as growing numbers posed for the world’s resources, the planet also faced other dangers. From 1950 to 2000 world industrial production grew more than fivefold, burning coal, oil, and natural gas, and consuming a variety of synthetic chemicals, all of which emitted heat-trapping gases into the atmosphere. Although the full implications remained controversial, many scientists and aroused citizens were convinced that these emissions were eroding the protective upper ozone layer in the atmosphere that shields the earth from excessive solar heat and harmful radiation, and that the resulting global warming of the climate would have serious consequences. One troubling sign was that the decade of the 1990s was the warmest globally in 600 years, and the warming trend continued in the new century. The ongoing debate prompted governments worldwide to convene periodic conferences to try to agree on regulatory measures.

Industrial pollution also caused the acid rain that laid waste to many forests, lakes, and rivers in the industrial nations. Severe environmental damage in the former Soviet Union, the Soviet-bloc nations, and the People’s Republic of China confirmed that pollution was not confined to any single economic system; central planners had pressed development with little regard for the ecological consequences. Developing nations also paid little attention to their environment. Under the pressure of expanding population and rapid urbanization, they permitted the slashing and burning of tropical forests for ranching, timber exports, and resettlement. Arable acreage declined; pasture land turned into desert. From 1950 to 2000 the world lost over one-fifth of its topsoil and one-fifth of its tropical rain forests. Plant and animal species were endangered. Nowhere was this more striking than in Latin America and Africa. The partial loss of the Amazon Forest in Brazil before preventive measures were taken dramatized the extent of the damage.

From the 1970s on a newly sensitized world began to speak of sustainable growth rates, growth that could be maintained without the destruction of humanity’s natural habitat. Alternative forms of energy, some as old as the sun and the wind, others new, like nuclear energy (with proper safeguards, to be sure) were proposed. Emissions from industrial factories and from vehicles came under stricter controls. The United States Environmental Protection Agency came into existence in 1970. In 1973 representatives of 80 nations signed an agreement to protect endangered species. In 1989 a number of industrial nations agreed to stop production of chlorofluorocarbons, which released dangerous amounts of chlorine into the upper atmosphere and were widely used in air-conditioning and in aerosol propellants. Other meetings and treaties followed. In 1992, at the first “Earth Summit” meeting in Rio de Janeiro, representatives of 178 countries pledged to protect plant and animal species and to take steps in a “climate treaty” to halt global warming. In 1997 representatives from 150 countries met at Kyoto, where they agreed to reduce the emission of so-called greenhouse gases by 50 percent by 2010.

As awareness of ecological dangers grew, grass-roots environmental organizations multiplied, and in several countries emerged as “Green” political parties. Despite the numerous pledges and measures to counter the threat of environmental deterioration, there was opposition to strict environmental regulations in some poor countries and in industrialized countries, including China, India, Australia, and the United States (which withdrew from the Kyoto agreement in 2001). Meanwhile, the developing nations slowed the adoption and enforcement of environmental laws that were viewed as impediments to economic progress.

Population growth and the threat to the environment added to the world’s concerns at the beginning of the twenty-first century and the new millennium. Many of the most pressing concerns, however, were not new: the implications for society of contemporary science and technology, the sovereignty of nations, peace among nations, and the quest within nations for freedom, dignity, and economic well-being. All were aspects of one overriding problem. How could human beings in each generation on earth, regardless of sex, color, creed, religion, nationality, or ethnic background—beings said by some to be made in the image of God, by others to have a natural right to life, liberty and happiness, by still others to have the freedom to create meaning in a meaningless universe—live out their lives in peace, fulfill their destiny, and pass on their heritage to future generations?

The constant changes and upheavals in contemporary human history might be compared to cataclysms in the natural world. A cataclysm is not a time of downfall only. Mountains crumble, but others are thrust up. Lands vanish, but others rise from the sea. So it is with the political and social cataclysm of our times. Old landmarks are worn down. Empires pass away; new nations arise in their place. Subjugated peoples regain independence; rigid ideologies collapse. The ascendancy of Europe, the West, and the European nations closes; they learn to negotiate with others, not to rule them. There is a greater fluidity in social relationships. Women and minorities struggle for equal places in society. But social justice and peace remain elusive goals. The gap between rich and poor among nations, and within nations, is not easily overcome. Old and new diseases, natural catastrophes, and armed conflicts exact their toll. Resurgent nationalism feeds on intolerance and hatred. Never has war been so potentially destructive; the menace of a nuclear war that would blight much of civilization wanes but does not disappear. Uncontrolled economic development threatens the environment, and population growth presses on natural resources. International cooperation and intervention are needed to protect human rights, end or prevent wars, and sustain the earth’s billions.

But there is a growing global recognition of all these concerns. To close this long history on a note of placidity would indeed be inappropriate, but it would also be wrong to close on a note of doom. The history of the modern world shows the astonishing range of human imagination and ingenuity, and there are good historical reasons to believe that people will continue to confront future problems and challenges with determination and creativity.